{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Claims Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import json as j\n",
    "from gb.hypergraph.hypergraph import HyperGraph\n",
    "import gb.hypergraph.symbol as sym\n",
    "import gb.hypergraph.edge as ed\n",
    "import gb.nlp.parser as par\n",
    "import gb.tools.json as json_tools\n",
    "from gb.synonyms.meronomy import Meronomy\n",
    "from gb.metrics.hyper_similarity import HyperSimilarity\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from visJS2jupyter import visJS_module\n",
    "from visJS2jupyter import visualizations\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_PROB = -12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hg = HyperGraph({'backend': 'leveldb', 'hg': '../reddit-worldnews-01012013-01082017.hg'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = par.Parser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aux Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rel_contains(full_edge, term):\n",
    "    if sym.is_edge(full_edge) and len(full_edge) > 2 and sym.is_edge(full_edge[2]):\n",
    "        rel = full_edge[0]\n",
    "        if sym.is_edge(rel):\n",
    "            return term in rel\n",
    "        else:\n",
    "            return rel == term\n",
    "    return False\n",
    "\n",
    "\n",
    "def edge2str(edge):\n",
    "    s = ed.edge2str(edge, namespaces=False)\n",
    "    if sym.is_edge(edge):\n",
    "        return s\n",
    "\n",
    "    if s[0] == '+':\n",
    "        s = s[1:]\n",
    "\n",
    "    if len(s) == 0:\n",
    "        return None\n",
    "\n",
    "    if not s[0].isalnum():\n",
    "        return None\n",
    "\n",
    "    word = parser.make_word(s)\n",
    "    if word.prob < MAX_PROB:\n",
    "        return s\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def edge2syn(edge):\n",
    "    atom = edge2str(edge)\n",
    "    if atom:\n",
    "        syn_id = mer.syn_id(atom)\n",
    "        if syn_id:\n",
    "            return syn_id\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meronomy and Say Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mer = Meronomy(parser)\n",
    "\n",
    "edge_data = json_tools.read('../reddit-worldnews-01012013-01082017-edges.json')\n",
    "\n",
    "say_edges = []\n",
    "for it in edge_data:\n",
    "    edge_ns = ed.str2edge(it['edge'])\n",
    "    mer.add_edge(edge_ns)\n",
    "    edge = ed.without_namespaces(edge_ns)\n",
    "    \n",
    "    # (says x ...)\n",
    "    if rel_contains(edge, 'says'):\n",
    "        say_edges.append(edge)\n",
    "        \n",
    "mer.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = Counter()\n",
    "\n",
    "for it in edge_data:\n",
    "    edge_ns = ed.str2edge(it['edge'])\n",
    "    # edge = ed.without_namespaces(edge_ns)\n",
    "    if sym.is_edge(edge_ns):\n",
    "        pred = ed.edge2str(edge_ns[0])\n",
    "        preds[pred] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('has_label/gb', 1816093), ('+/gb', 1813382), ('says/nlp.say.verb', 12250), ('is/nlp.be.verb', 6045), ('to/nlp.to.part', 4109), ('said/nlp.say.verb', 3523), ('killed/nlp.kill.verb', 3465), ('kills/nlp.kill.verb', 3147), ('say/nlp.say.verb', 2913), ('warns/nlp.warn.verb', 2264), ('calls/nlp.call.verb', 1841), ('are/nlp.be.verb', 1775), ('found/nlp.find.verb', 1759), ('kill/nlp.kill.verb', 1601), ('was/nlp.be.verb', 1401), ('has/nlp.have.verb', 1346), ('arrested/nlp.arrest.verb', 1299), ('hits/nlp.hit.verb', 1103), ('finds/nlp.find.verb', 1082), ('urges/nlp.urge.verb', 1073), ('shows/nlp.show.verb', 1065), ('dies/nlp.die.verb', 1055), ('rejects/nlp.reject.verb', 1008), ('accuses/nlp.accuse.verb', 982), ('faces/nlp.face.verb', 976), ('tells/nlp.tell.verb', 954), ('threatens/nlp.threaten.verb', 940), ('claims/nlp.claim.verb', 929), ('gets/nlp.get.verb', 925), ('wants/nlp.want.verb', 918), ('seeks/nlp.seek.verb', 913), ('takes/nlp.take.verb', 894), ('makes/nlp.make.verb', 849), ('hit/nlp.hit.verb', 808), ('announces/nlp.announce.verb', 798), ('wins/nlp.win.verb', 761), ('reveals/nlp.reveal.verb', 755), ('confirms/nlp.confirm.verb', 747), ('approves/nlp.approve.verb', 740), ('denies/nlp.deny.verb', 729), ('bans/nlp.ban.verb', 691), ('sends/nlp.send.verb', 682), ('gives/nlp.give.verb', 674), ('condemns/nlp.condemn.verb', 673), ('not/nlp.not.adv', 670), ('offers/nlp.offer.verb', 653), ('show/nlp.show.verb', 648), ('declares/nlp.declare.verb', 647), ('slams/nlp.slam.verb', 614), ('opens/nlp.open.verb', 603), ('asks/nlp.ask.verb', 598), ('shot/nlp.shoot.verb', 595), ('have/nlp.have.verb', 585), ('sees/nlp.see.verb', 585), ('take/nlp.take.verb', 561), ('begins/nlp.begin.verb', 555), ('leaves/nlp.leave.verb', 547), ('goes/nlp.go.verb', 541), ('admits/nlp.admit.verb', 531), ('raises/nlp.raise.verb', 525), ('reported/nlp.report.verb', 523), (\"'s/nlp.'.verb\", 518), ('jailed/nlp.jail.verb', 511), ('suspends/nlp.suspend.verb', 509), ('told/nlp.tell.verb', 495), ('find/nlp.find.verb', 493), ('protest/nlp.protest.verb', 490), ('becomes/nlp.become.verb', 489), ('face/nlp.face.verb', 487), ('(says/nlp.say.verb is/nlp.be.verb)', 485), ('vows/nlp.vow.verb', 485), ('has_said/nlp.have.verb+nlp.say.verb', 482), ('passes/nlp.pass.verb', 474), ('accused/nlp.accuse.verb', 470), ('blames/nlp.blame.verb', 452), ('set/nlp.set.verb', 450), ('suggests/nlp.suggest.verb', 448), ('resigns/nlp.resign.verb', 438), ('charged/nlp.charge.verb', 430), ('call/nlp.call.verb', 429), ('seize/nlp.seize.verb', 427), ('loses/nlp.lose.verb', 426), ('holds/nlp.hold.verb', 420), ('puts/nlp.put.verb', 418), ('turns/nlp.turn.verb', 418), ('were/nlp.be.verb', 418), ('agree/nlp.agree.verb', 417), ('defends/nlp.defend.verb', 416), ('comes/nlp.come.verb', 415), ('warn/nlp.warn.verb', 413), ('attack/nlp.attack.verb', 401), ('sentenced/nlp.sentence.verb', 394), ('detained/nlp.detain.verb', 391), ('had/nlp.have.verb', 391), ('unveils/nlp.unveil.verb', 383), ('die/nlp.die.verb', 382), ('attacked/nlp.attack.verb', 379), ('rises/nlp.rise.verb', 376), ('warned/nlp.warn.verb', 373), ('arrest/nlp.arrest.verb', 370)]\n"
     ]
    }
   ],
   "source": [
    "print(preds.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('opens/nlp.open.verb',\n",
       "  'russia/nlp.russia.noun',\n",
       "  ('+/gb',\n",
       "   'criminal/nlp.criminal.adj',\n",
       "   'case/nlp.case.noun',\n",
       "   ('+/gb', 'against/nlp.against.adp', 'us/nlp.us.pron'),\n",
       "   ('+/gb',\n",
       "    'band/nlp.band.noun',\n",
       "    ('+/gb', 'bloodhound/nlp.bloodhound.noun', 'gang/nlp.gang.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'turkish/nlp.turkish.adj', 'tunnel/nlp.tunnel.noun'),\n",
       "  ('+/gb',\n",
       "   'first/nlp.first.adj',\n",
       "   ('+/gb',\n",
       "    'undersea/nlp.undersea.adj',\n",
       "    ('+/gb',\n",
       "     'between/nlp.between.adp',\n",
       "     'link/nlp.link.noun',\n",
       "     ('+/gb', 'two/nlp.two.num', 'continents/nlp.continent.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'berlin/nlp.berlin.noun',\n",
       "  ('+/gb',\n",
       "   'first/nlp.first.adj',\n",
       "   ('+/gb',\n",
       "    'lgbt/nlp.lgbt.adj',\n",
       "    ('+/gb', 'refugee/nlp.refugee.noun', 'center/nlp.center.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'microsoft/nlp.microsoft.noun',\n",
       "  ('+/gb',\n",
       "   'third/nlp.third.adj',\n",
       "   'center/nlp.center.noun',\n",
       "   ('+/gb',\n",
       "    'and/nlp.and.conj',\n",
       "    ('+/gb',\n",
       "     'development/nlp.development.noun',\n",
       "     'research/nlp.research.noun')),\n",
       "   ('+/gb', 'in/nlp.in.adp', 'israel/nlp.israel.propn'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   \"'s/nlp.'s.part\",\n",
       "   ('+/gb', 'hariri/nlp.hariri.noun', 'trial/nlp.trial.noun'),\n",
       "   'lebanon/nlp.lebanon.noun'),\n",
       "  ('+/gb',\n",
       "   'without/nlp.without.adp',\n",
       "   ('+/gb', 'the/nlp.the.det', 'accused/nlp.accuse.verb'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'the/nlp.the.det', 'finding/nlp.finding.noun'),\n",
       "  ('+/gb',\n",
       "   ('+/gb', 'a/nlp.a.det', 'path/nlp.path.noun'),\n",
       "   ('to_better/nlp.to.part+nlp.better.adv',\n",
       "    ('+/gb',\n",
       "     'and/nlp.and.conj',\n",
       "     ('+/gb',\n",
       "      'understanding/nlp.understanding.noun',\n",
       "      'monitoring/nlp.monitoring.noun')),\n",
       "    ('+/gb',\n",
       "     'of/nlp.of.adp',\n",
       "     ('how_is_changing/nlp.how.adv+nlp.be.verb+nlp.change.verb',\n",
       "      ('+/gb', 'ocean/nlp.ocean.adj', 'circulation/nlp.circulation.noun')))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'ferrari/nlp.ferrari.adj',\n",
       "   ('+/gb',\n",
       "    'land/nlp.land.noun',\n",
       "    ('+/gb', 'theme/nlp.theme.noun', 'park/nlp.park.noun'))),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'spain/nlp.spain.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   \"'s/nlp.'s.part\",\n",
       "   ('+/gb',\n",
       "    'largest/nlp.large.adj',\n",
       "    ('+/gb', 'ice/nlp.ice.noun', 'maze/nlp.maze.noun')),\n",
       "   'world/nlp.world.noun'),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'poland/nlp.poland.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'crippled/nlp.crippled.adj',\n",
       "   ('+/gb', 'nuclear/nlp.nuclear.adj', 'plant/nlp.plant.noun')),\n",
       "  'doors/nlp.door.noun'),\n",
       " ('opens/nlp.open.verb',\n",
       "  'turkey/nlp.turkey.noun',\n",
       "  ('+/gb',\n",
       "   'first/nlp.first.adj',\n",
       "   ('+/gb',\n",
       "    'african/nlp.african.adj',\n",
       "    ('+/gb',\n",
       "     'military/nlp.military.adj',\n",
       "     ('+/gb',\n",
       "      'in/nlp.in.adp',\n",
       "      'base/nlp.base.noun',\n",
       "      'somalia/nlp.somalia.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'germany/nlp.germany.noun',\n",
       "  ('+/gb',\n",
       "   'first/nlp.first.adj',\n",
       "   ('+/gb',\n",
       "    'for/nlp.for.adp',\n",
       "    'shelter/nlp.shelter.noun',\n",
       "    ('+/gb', 'gay/nlp.gay.noun', 'refugees/nlp.refugee.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'honda/nlp.honda.noun',\n",
       "  ('+/gb',\n",
       "   'new/nlp.new.adj',\n",
       "   ('+/gb',\n",
       "    'in/nlp.in.adp',\n",
       "    'station/nlp.station.noun',\n",
       "    'swindon/nlp.swindon.noun'),\n",
       "   ('+/gb', 'hydrogen/nlp.hydrogen.noun', 'filling/nlp.filling.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'disgraced/nlp.disgraced.adj',\n",
       "   'cahuzac/nlp.cahuzac.noun',\n",
       "   ('+/gb', 'former/nlp.former.adj', 'minister/nlp.minister.noun')),\n",
       "  ('+/gb',\n",
       "   'french/nlp.french.adj',\n",
       "   ('+/gb', 'of/nlp.of.adp', 'trial/nlp.trial.noun'),\n",
       "   ('+/gb', 'tax/nlp.tax.noun', 'fraud/nlp.fraud.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'french/nlp.french.adj', 'prosecutor/nlp.prosecutor.noun'),\n",
       "  ('+/gb',\n",
       "   'fiat/nlp.fiat.noun',\n",
       "   'investigation/nlp.investigation.noun',\n",
       "   ('+/gb', 'chrysler/nlp.chrysler.noun', 'emissions/nlp.emission.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'north/nlp.north.noun',\n",
       "   ('+/gb',\n",
       "    'luxury/nlp.luxury.noun',\n",
       "    ('+/gb', 'ski/nlp.ski.noun', 'resort/nlp.resort.noun')),\n",
       "   ('+/gb', \"'s/nlp.'s.part\", 'korea/nlp.korea.noun')),\n",
       "  ('+/gb', 'for/nlp.for.adp', 'business/nlp.business.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'kim/nlp.kim.noun',\n",
       "   ('+/gb',\n",
       "    'dotcom/nlp.dotcom.noun',\n",
       "    ('+/gb', 'extradition/nlp.extradition.noun', 'hearing/nlp.hearing.noun'))),\n",
       "  ('+/gb',\n",
       "   'in/nlp.in.adp',\n",
       "   ('+/gb', 'new/nlp.new.adj', 'zealand/nlp.zealand.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'eu/nlp.eu.noun',\n",
       "  ('+/gb',\n",
       "   'against/nlp.against.adp',\n",
       "   ('+/gb',\n",
       "    'over/nlp.over.adp',\n",
       "    'case/nlp.case.noun',\n",
       "    ('+/gb',\n",
       "     'new/nlp.new.adj',\n",
       "     'media/nlp.medium.noun',\n",
       "     ('+/gb', 'court/nlp.court.noun', 'laws/nlp.law.noun'))),\n",
       "   'poland/nlp.poland.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'germany/nlp.germany.noun',\n",
       "  ('+/gb',\n",
       "   'criminal/nlp.criminal.adj',\n",
       "   ('+/gb',\n",
       "    'against/nlp.against.adp',\n",
       "    'probe/nlp.probe.noun',\n",
       "    ('+/gb',\n",
       "     'zdf/nlp.zdf.noun',\n",
       "     ('+/gb', 'tv/nlp.tv.noun', 'presenter/nlp.presenter.noun'),\n",
       "     ('reciting/nlp.recite.verb',\n",
       "      ('+/gb', 'defamatory/nlp.defamatory.adj', 'poem/nlp.poem.noun'),\n",
       "      ('+/gb', 'insulting/nlp.insult.verb', 'erdoğan/nlp.erdoğan.noun')))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'new/nlp.new.adj',\n",
       "   ('+/gb', 'italian/nlp.italian.adj', 'pm/nlp.pm.noun')),\n",
       "  ('+/gb',\n",
       "   'talks/nlp.talk.noun',\n",
       "   ('+/gb',\n",
       "    'on/nlp.on.adp',\n",
       "    ('forming/nlp.form.verb',\n",
       "     ('+/gb',\n",
       "      'a/nlp.a.det',\n",
       "      ('+/gb',\n",
       "       'workable/nlp.workable.adj',\n",
       "       'government/nlp.government.noun')))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'france/nlp.france.noun', 'probe/nlp.probe.noun'),\n",
       "  ('+/gb',\n",
       "   'into/nlp.into.adp',\n",
       "   ('+/gb',\n",
       "    'alleged/nlp.allege.verb',\n",
       "    ('+/gb',\n",
       "     'libyan/nlp.libyan.adj',\n",
       "     ('+/gb',\n",
       "      'to/nlp.to.adp',\n",
       "      'aid/nlp.aid.noun',\n",
       "      'sarkozy/nlp.sarkozy.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'rwanda/nlp.rwanda.noun',\n",
       "   ('+/gb', 'genocide/nlp.genocide.noun', 'trial/nlp.trial.noun')),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'france/nlp.france.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'nasa/nlp.nasa.adj',\n",
       "   ('+/gb', 'media/nlp.medium.noun', 'accreditation/nlp.accreditation.noun')),\n",
       "  ('+/gb',\n",
       "   'for/nlp.for.adp',\n",
       "   ('+/gb',\n",
       "    'of/nlp.of.adp',\n",
       "    'launch/nlp.launch.noun',\n",
       "    ('+/gb',\n",
       "     'next/nlp.next.adj',\n",
       "     ('+/gb',\n",
       "      ('+/gb', 'spacex/nlp.spacex.noun', 'station/nlp.station.noun'),\n",
       "      'mission/nlp.mission.noun',\n",
       "      'resupply/nlp.resupply.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'putin/nlp.putin.noun',\n",
       "  ('+/gb',\n",
       "   ('+/gb', 'of/nlp.of.adp', 'one/nlp.one.num'),\n",
       "   ('europe/nlp.europe.adv',\n",
       "    ('+/gb',\n",
       "     '’s/nlp.’s.adj',\n",
       "     ('+/gb',\n",
       "      'biggest/nlp.big.adj',\n",
       "      ('+/gb',\n",
       "       'in/nlp.in.adp',\n",
       "       'mosques/nlp.mosque.noun',\n",
       "       'russia/nlp.russia.noun')))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'julian/nlp.julian.adj',\n",
       "   'assange/nlp.assange.noun',\n",
       "   ('+/gb', 'wikileaks/nlp.wikileaks.adj', 'party/nlp.party.noun')),\n",
       "  ('+/gb',\n",
       "   'for/nlp.for.adp',\n",
       "   ('+/gb',\n",
       "    'in/nlp.in.adp',\n",
       "    'membership/nlp.membership.noun',\n",
       "    'australia/nlp.australia.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'first/nlp.first.adv',\n",
       "   'mosque/nlp.mosque.noun',\n",
       "   ('+/gb', 'lgbt/nlp.lgbt.noun', 'friendly/nlp.friendly.adj')),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'france/nlp.france.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'ethiopia/nlp.ethiopia.noun',\n",
       "  ('+/gb',\n",
       "   'massive/nlp.massive.adj',\n",
       "   ('+/gb',\n",
       "    '3/nlp.3.num',\n",
       "    'gibe/nlp.gibe.noun',\n",
       "    ('+/gb',\n",
       "     'hydroelectric/nlp.hydroelectric.adj',\n",
       "     ('+/gb',\n",
       "      'on/nlp.on.adp',\n",
       "      'dam/nlp.dam.noun',\n",
       "      ('+/gb', 'omo/nlp.omo.adj', 'river/nlp.river.noun')))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'india/nlp.india.noun',\n",
       "  ('+/gb',\n",
       "   \"'s/nlp.'s.part\",\n",
       "   ('ever/nlp.ever.adv',\n",
       "    ('+/gb',\n",
       "     'solar/nlp.solar.adj',\n",
       "     ('+/gb', 'first/nlp.first.adj', 'airport/nlp.airport.noun'),\n",
       "     'powered/nlp.powered.adj')),\n",
       "   'world/nlp.world.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'the/nlp.the.det',\n",
       "   ('+/gb',\n",
       "    'of/nlp.of.adp',\n",
       "    'power/nlp.power.noun',\n",
       "    ('+/gb',\n",
       "     'deaf/nlp.deaf.adj',\n",
       "     ('+/gb',\n",
       "      'education/nlp.education.noun',\n",
       "      ('+/gb',\n",
       "       'deaf/nlp.deaf.adj',\n",
       "       'school/nlp.school.noun',\n",
       "       'first/nlp.first.adj'))))),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'ethiopia/nlp.ethiopia.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'in/nlp.in.adp',\n",
       "   'francis/nlp.francis.noun',\n",
       "   'pope/nlp.pope.noun',\n",
       "   ('+/gb',\n",
       "    ('+/gb', 'foot/nlp.foot.noun', 'washing/nlp.washing.noun'),\n",
       "    'rite/nlp.rite.noun'),\n",
       "   ('+/gb',\n",
       "    'to/nlp.to.adp',\n",
       "    ('+/gb',\n",
       "     'in/nlp.in.adp',\n",
       "     'women/nlp.woman.noun',\n",
       "     ('+/gb',\n",
       "      'of/nlp.of.adp',\n",
       "      'gesture/nlp.gesture.noun',\n",
       "      'inclusion/nlp.inclusion.noun'))),\n",
       "   ('+/gb',\n",
       "    ('+/gb', 'a/nlp.a.det', 'change/nlp.change.noun'),\n",
       "    ('that_quickly_set_buzzing/nlp.quickly.adv+nlp.set.verb+nlp.buzz.verb+nlp.that.adj',\n",
       "     ('+/gb',\n",
       "      'the/nlp.the.det',\n",
       "      ('+/gb', 'catholic/nlp.catholic.adj', 'world/nlp.world.noun'))))),\n",
       "  ('+/gb',\n",
       "   'allowed/nlp.allow.verb',\n",
       "   ('+/gb', 'pope/nlp.pope.noun', 'francis/nlp.francis.noun'),\n",
       "   ('to_wash/nlp.to.part+nlp.wash.verb',\n",
       "    'priests/nlp.priest.noun',\n",
       "    ('+/gb',\n",
       "     '&/nlp.&.conj',\n",
       "     ('+/gb',\n",
       "      ('+/gb', \"'s/nlp.'s.part\", 'women/nlp.woman.noun'),\n",
       "      'feet/nlp.foot.noun'),\n",
       "     ('else/nlp.else.adv',\n",
       "      ('+/gb',\n",
       "       'in/nlp.in.adp',\n",
       "       'anyone/nlp.anyone.noun',\n",
       "       ('+/gb', 'the/nlp.the.det', 'community/nlp.community.noun')))),\n",
       "    ('+/gb',\n",
       "     'on/nlp.on.adp',\n",
       "     ('+/gb',\n",
       "      'holy/nlp.holy.adj',\n",
       "      ('+/gb', '&/nlp.&.conj', 'thursday/nlp.thursday.noun'),\n",
       "      ('not_just/nlp.not.adv+nlp.just.adv', 'men/nlp.man.noun'))),\n",
       "    ('+/gb',\n",
       "     'as/nlp.as.adp',\n",
       "     'previously_decreed/nlp.previously.adv+nlp.decree.verb',\n",
       "     ('+/gb', 'church/nlp.church.noun', 'law/nlp.law.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'iran/nlp.iran.noun',\n",
       "  ('+/gb',\n",
       "   'gas/nlp.ga.noun',\n",
       "   ('+/gb',\n",
       "    'operations/nlp.operation.noun',\n",
       "    ('+/gb', '$/nlp.$.sym', 'billion/nlp.billion.num', '6/nlp.6.num')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'sa/nlp.sa.noun',\n",
       "  ('+/gb',\n",
       "   'the/nlp.the.det',\n",
       "   ('+/gb',\n",
       "    'sefako/nlp.sefako.noun',\n",
       "    ('+/gb', 'makgatho/nlp.makgatho.adj', 'university/nlp.university.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'venezuela/nlp.venezuela.noun',\n",
       "   ('+/gb', '’s/nlp.’s.propn', 'congress/nlp.congress.noun')),\n",
       "  ('+/gb',\n",
       "   'a/nlp.a.det',\n",
       "   ('+/gb',\n",
       "    'political/nlp.political.adj',\n",
       "    ('+/gb',\n",
       "     'against/nlp.against.adp',\n",
       "     'trial/nlp.trial.noun',\n",
       "     ('+/gb',\n",
       "      'president/nlp.president.noun',\n",
       "      'maduro/nlp.maduro.noun',\n",
       "      'nicolas/nlp.nicolas.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'first/nlp.first.adj',\n",
       "   ('+/gb',\n",
       "    'palestinian/nlp.palestinian.adj',\n",
       "    ('+/gb',\n",
       "     'in/nlp.in.adp',\n",
       "     'embassy/nlp.embassy.noun',\n",
       "     ('+/gb', 'western/nlp.western.adj', 'europe/nlp.europe.noun')))),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'sweden/nlp.sweden.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'forest/nlp.forest.noun',\n",
       "   ('+/gb', 'deep/nlp.deep.adj', 'hole/nlp.hole.noun'),\n",
       "   'hill/nlp.hill.noun',\n",
       "   '13-ft/nlp.13-ft.num'),\n",
       "  ('+/gb',\n",
       "   'under/nlp.under.adp',\n",
       "   ('+/gb',\n",
       "    'railway/nlp.railway.noun',\n",
       "    'news/nlp.news.noun',\n",
       "    ('+/gb', 'line/nlp.line.noun', 'bbc/nlp.bbc.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   \"'s/nlp.'s.part\",\n",
       "   ('+/gb',\n",
       "    'first/nlp.first.adj',\n",
       "    ('+/gb', 'gas/nlp.ga.noun', 'station/nlp.station.noun')),\n",
       "   'uae/nlp.uae.noun',\n",
       "   ('+/gb', 'solar/nlp.solar.noun', 'powered/nlp.power.verb')),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'dubai/nlp.dubai.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'the/nlp.the.det',\n",
       "   ('+/gb', 'highest/nlp.high.adj', 'bridge/nlp.bridge.noun'),\n",
       "   ('+/gb', \"'s/nlp.'s.part\", 'world/nlp.world.noun')),\n",
       "  ('+/gb', 'for/nlp.for.adp', 'traffic/nlp.traffic.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'thaad/nlp.thaad.det', 'base/nlp.base.noun'),\n",
       "  ('+/gb',\n",
       "   'to/nlp.to.adp',\n",
       "   ('+/gb', 'korean/nlp.korean.adj', 'press/nlp.press.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'turner/nlp.turner.det',\n",
       "   ('+/gb', 'prize/nlp.prize.noun', 'exhibition/nlp.exhibition.noun')),\n",
       "  ('+/gb',\n",
       "   'with/nlp.with.adp',\n",
       "   ('+/gb',\n",
       "    'giant/nlp.giant.adj',\n",
       "    'buttocks/nlp.buttock.noun',\n",
       "    ('+/gb',\n",
       "     'train/nlp.train.noun',\n",
       "     ('+/gb', 'and/nlp.and.conj', 'rides/nlp.ride.noun'),\n",
       "     ('+/gb',\n",
       "      'scrap/nlp.scrap.noun',\n",
       "      'news/nlp.news.noun',\n",
       "      ('+/gb', 'metal/nlp.metal.noun', 'bbc/nlp.bbc.noun')))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'international/nlp.international.adj',\n",
       "   'conference/nlp.conference.noun',\n",
       "   ('+/gb', 'islamic/nlp.islamic.adj', 'unity/nlp.unity.noun')),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'tehran/nlp.tehran.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'data/nlp.datum.noun', 'collection/nlp.collection.noun'),\n",
       "  ('+/gb',\n",
       "   'to/nlp.to.adp',\n",
       "   'door/nlp.door.noun',\n",
       "   ('+/gb',\n",
       "    'and/nlp.and.conj',\n",
       "    'abuses/nlp.abuse.noun',\n",
       "    ('+/gb',\n",
       "     ('+/gb', 'discrimination/nlp.discrimination.noun', 'us/nlp.us.pron'),\n",
       "     'experts/nlp.expert.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   \"'s/nlp.'s.part\",\n",
       "   'trudeau/nlp.trudeau.noun',\n",
       "   'canada/nlp.canada.noun'),\n",
       "  ('+/gb',\n",
       "   'talks/nlp.talk.noun',\n",
       "   ('+/gb',\n",
       "    'with/nlp.with.adp',\n",
       "    ('aiming/nlp.aim.verb',\n",
       "     'trump/nlp.trump.noun',\n",
       "     ('to_boost/nlp.to.part+nlp.boost.verb', 'trade/nlp.trade.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'successful/nlp.successful.adj',\n",
       "   ('+/gb', 'rocket/nlp.rocket.noun', 'launch/nlp.launch.noun')),\n",
       "  ('+/gb',\n",
       "   'new/nlp.new.adj',\n",
       "   ('+/gb',\n",
       "    'for/nlp.for.adp',\n",
       "    'chapter/nlp.chapter.noun',\n",
       "    ('+/gb', 's./nlp.s..noun', 'korea/nlp.korea.noun'),\n",
       "    ('|/nlp.|.part',\n",
       "     ('+/gb', 'yonhap/nlp.yonhap.noun', 'news/nlp.news.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'thailand/nlp.thailand.noun',\n",
       "  ('+/gb',\n",
       "   'into/nlp.into.adp',\n",
       "   ('+/gb',\n",
       "    'for/nlp.for.adp',\n",
       "    'investigation/nlp.investigation.noun',\n",
       "    ('+/gb',\n",
       "     'alleged/nlp.alleged.adj',\n",
       "     ('+/gb',\n",
       "      'of/nlp.of.adp',\n",
       "      'insult/nlp.insult.noun',\n",
       "      ('+/gb', 'new/nlp.new.adj', 'king/nlp.king.noun')))),\n",
       "   'bbc/nlp.bbc.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'jordan/nlp.jordan.noun',\n",
       "  ('+/gb',\n",
       "   'syria/nlp.syria.adj',\n",
       "   ('+/gb',\n",
       "    'refugee/nlp.refugee.noun',\n",
       "    ('+/gb',\n",
       "     'for/nlp.for.adp',\n",
       "     'camp/nlp.camp.noun',\n",
       "     ('+/gb', '130,000/nlp.130,000.num', 'people/nlp.people.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'egypt/nlp.egypt.noun',\n",
       "  ('+/gb',\n",
       "   'first/nlp.first.adj',\n",
       "   ('+/gb',\n",
       "    'solar/nlp.solar.noun',\n",
       "    ('+/gb',\n",
       "     'gas/nlp.ga.noun',\n",
       "     'station/nlp.station.noun',\n",
       "     'powered/nlp.power.verb')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   \"'s/nlp.'s.part\",\n",
       "   ('+/gb',\n",
       "    'first/nlp.first.adj',\n",
       "    ('+/gb', 'zoo/nlp.zoo.noun', 'microbe/nlp.microbe.noun')),\n",
       "   'world/nlp.world.noun'),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'amsterdam/nlp.amsterdam.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'vladimir/nlp.vladimir.noun', 'putin/nlp.putin.noun'),\n",
       "  'russian/nlp.russian.adj'),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'paris/nlp.paris.adj', 'prosecutor/nlp.prosecutor.noun'),\n",
       "  ('+/gb',\n",
       "   ('+/gb', 'involuntary/nlp.involuntary.adj', 'homicide/nlp.homicide.noun'),\n",
       "   'investigation/nlp.investigation.noun',\n",
       "   ('+/gb',\n",
       "    'into/nlp.into.adp',\n",
       "    ('+/gb', 'egyptair/nlp.egyptair.propn', 'crash/nlp.crash.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'nazi/nlp.nazi.adj', 'cafe/nlp.cafe.noun'),\n",
       "  ('+/gb',\n",
       "   'in/nlp.in.adp',\n",
       "   ('+/gb', 'indonesia/nlp.indonesia.noun', 'bandung/nlp.bandung.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'eu/nlp.eu.noun',\n",
       "  ('+/gb',\n",
       "   'on/nlp.on.adp',\n",
       "   'debate/nlp.debate.noun',\n",
       "   ('+/gb',\n",
       "    'china/nlp.china.noun',\n",
       "    ('+/gb', 'trade/nlp.trade.noun', 'status/nlp.status.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'denmark/nlp.denmark.noun',\n",
       "  ('+/gb',\n",
       "   ('+/gb',\n",
       "    'first/nlp.first.adj',\n",
       "    ('+/gb',\n",
       "     'selling/nlp.selling.noun',\n",
       "     ('+/gb',\n",
       "      ('+/gb', 'food/nlp.food.noun', 'waste/nlp.waste.noun'),\n",
       "      'surplus/nlp.surplus.noun',\n",
       "      'supermarket/nlp.supermarket.noun'))),\n",
       "   'produce/nlp.produce.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'china/nlp.china.noun',\n",
       "  ('+/gb',\n",
       "   'new/nlp.new.adj',\n",
       "   ('+/gb',\n",
       "    'land/nlp.land.noun',\n",
       "    ('+/gb',\n",
       "     'for/nlp.for.adp',\n",
       "     ('+/gb',\n",
       "      'to/nlp.to.adp',\n",
       "      'route/nlp.route.noun',\n",
       "      ('+/gb', 'indian/nlp.indian.adj', 'pilgrims/nlp.pilgrim.noun'),\n",
       "      'tibet/nlp.tibet.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'new/nlp.new.adj',\n",
       "   ('+/gb',\n",
       "    'of/nlp.of.adp',\n",
       "    'extension/nlp.extension.noun',\n",
       "    ('+/gb',\n",
       "     'the/nlp.the.det',\n",
       "     ('+/gb', 'suez/nlp.suez.adj', 'canal/nlp.canal.noun')))),\n",
       "  ('+/gb',\n",
       "   'with/nlp.with.adp',\n",
       "   ('+/gb',\n",
       "    'celebrations/nlp.celebration.noun',\n",
       "    ('hosted_by/nlp.host.verb+nlp.by.adp',\n",
       "     ('+/gb', 'el/nlp.el.propn', 'sisi/nlp.sisi.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'uk/nlp.uk.pron',\n",
       "   ('+/gb',\n",
       "    'first/nlp.first.adj',\n",
       "    ('+/gb', 'cannabis/nlp.cannabis.adj', 'pharmacy/nlp.pharmacy.noun')),\n",
       "   \"'s/nlp.'s.part\"),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'london/nlp.london.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'samsung/nlp.samsung.adj',\n",
       "   ('+/gb',\n",
       "    'global/nlp.global.adj',\n",
       "    ('+/gb', 'innovation/nlp.innovation.noun', 'center/nlp.center.noun'))),\n",
       "  ('+/gb',\n",
       "   'tel/nlp.tel.noun',\n",
       "   ('+/gb', 'aviv/nlp.aviv.noun', 'branch/nlp.branch.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'of/nlp.of.adp',\n",
       "   'trial/nlp.trial.noun',\n",
       "   ('+/gb',\n",
       "    'prominent/nlp.prominent.adj',\n",
       "    'lawyer/nlp.lawyer.noun',\n",
       "    'rights/nlp.right.noun',\n",
       "    ('+/gb',\n",
       "     'who/nlp.who.noun',\n",
       "     'was_reportedly_tortured/nlp.be.verb+nlp.reportedly.adv+nlp.torture.verb',\n",
       "     ('+/gb', 'in/nlp.in.adp', 'custody/nlp.custody.noun')))),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'china/nlp.china.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'pakistan/nlp.pakistan.noun',\n",
       "  ('+/gb',\n",
       "   'new/nlp.new.adj',\n",
       "   ('+/gb',\n",
       "    'nuclear/nlp.nuclear.adj',\n",
       "    ('built/nlp.build.verb',\n",
       "     'plant/nlp.plant.noun',\n",
       "     ('with/nlp.with.adp',\n",
       "      ('+/gb',\n",
       "       \"'s/nlp.'s.part\",\n",
       "       'help/nlp.help.noun',\n",
       "       'china/nlp.china.noun')))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'european/nlp.european.adj', 'parliament/nlp.parliament.noun'),\n",
       "  'inquiry/nlp.inquiry.noun'),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'first/nlp.first.adj',\n",
       "   ('+/gb',\n",
       "    'pre/nlp.pre.noun',\n",
       "    ('+/gb',\n",
       "     'for/nlp.for.adp',\n",
       "     'school/nlp.school.noun',\n",
       "     'entrepreneurs/nlp.entrepreneur.noun'))),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'belgium/nlp.belgium.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'an/nlp.an.det',\n",
       "   ('+/gb',\n",
       "    'expanded/nlp.expand.verb',\n",
       "    ('+/gb', 'panama/nlp.panama.noun', 'canal/nlp.canal.noun'))),\n",
       "  ('+/gb',\n",
       "   'for/nlp.for.adp',\n",
       "   ('+/gb', 'giant/nlp.giant.adj', 'ships/nlp.ship.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'china/nlp.china.noun',\n",
       "  ('+/gb',\n",
       "   'highest/nlp.high.adj',\n",
       "   ('+/gb',\n",
       "    'bridge/nlp.bridge.noun',\n",
       "    ('+/gb',\n",
       "     'the/nlp.the.det',\n",
       "     ('+/gb', \"'s/nlp.'s.part\", 'world/nlp.world.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'rice/nlp.rice.noun',\n",
       "   ('+/gb',\n",
       "    'husk/nlp.husk.noun',\n",
       "    ('+/gb', 'power/nlp.power.noun', 'plant/nlp.plant.noun'))),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'myanmar/nlp.myanmar.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'turkey/nlp.turkey.noun',\n",
       "  ('+/gb',\n",
       "   \"'s/nlp.'s.part\",\n",
       "   ('+/gb', 'widest/nlp.wide.adj', 'bridge/nlp.bridge.noun'),\n",
       "   'world/nlp.world.noun',\n",
       "   (\"'/nlp.'.part\", 'suspension/nlp.suspension.noun'),\n",
       "   ('linking/nlp.link.verb',\n",
       "    'asia/nlp.asia.noun',\n",
       "    ('+/gb', 'to/nlp.to.adp', 'europe/nlp.europe.verb')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   \"'s/nlp.'s.part\",\n",
       "   ('+/gb',\n",
       "    'spectacular/nlp.spectacular.adj',\n",
       "    ('+/gb',\n",
       "     '€/nlp.€.propn',\n",
       "     'elbphilharmonie/nlp.elbphilharmonie.noun',\n",
       "     '800/nlp.800.num')),\n",
       "   'hamburg/nlp.hamburg.noun',\n",
       "   'm/nlp.m.num'),\n",
       "  ('+/gb',\n",
       "   'for/nlp.for.adp',\n",
       "   ('+/gb', 'first/nlp.first.adj', 'concert/nlp.concert.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'saudi/nlp.saudi.adj',\n",
       "  ('+/gb',\n",
       "   ('+/gb', 'luxury/nlp.luxury.noun', 'rehab/nlp.rehab.noun'),\n",
       "   'centre/nlp.centre.noun',\n",
       "   ('+/gb',\n",
       "    'for/nlp.for.adp',\n",
       "    ('+/gb',\n",
       "     'militants/nlp.militant.noun',\n",
       "     ('+/gb', 'al/nlp.al.propn', 'qaeda/nlp.qaeda.propn'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'germany/nlp.germany.noun',\n",
       "  ('+/gb',\n",
       "   'hearings/nlp.hearing.noun',\n",
       "   ('+/gb', 'on/nlp.on.adp', 'u.s/nlp.u.s.propn'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'billion/nlp.billion.num',\n",
       "   ('eco/nlp.eco.adv',\n",
       "    ('+/gb',\n",
       "     'train/nlp.train.noun',\n",
       "     ('+/gb', 'friendly/nlp.friendly.adj', 'station/nlp.station.noun'))),\n",
       "   'euro/nlp.euro.noun'),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'vienna/nlp.vienna.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'nato/nlp.nato.noun',\n",
       "  ('+/gb',\n",
       "   'its/nlp.its.adj',\n",
       "   ('+/gb',\n",
       "    'biggest/nlp.big.adj',\n",
       "    ('+/gb',\n",
       "     'joint/nlp.joint.adj',\n",
       "     ('+/gb',\n",
       "      'in/nlp.in.adp',\n",
       "      'drill/nlp.drill.noun',\n",
       "      'poland/nlp.poland.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'venezuela/nlp.venezuela.noun',\n",
       "  ('to/nlp.to.part',\n",
       "   ('+/gb',\n",
       "    'disarmament/nlp.disarmament.noun',\n",
       "    ('+/gb', \"'/nlp.'.part\", 'centers/nlp.center.noun')),\n",
       "   'take/nlp.take.verb',\n",
       "   'guns/nlp.gun.noun',\n",
       "   ('+/gb', 'from/nlp.from.adp', 'civilians/nlp.civilian.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   '1100-mile/nlp.1100-mile.adj',\n",
       "   ('+/gb', 'rail/nlp.rail.noun', 'route/nlp.route.noun'),\n",
       "   ('+/gb', 'high/nlp.high.adj', 'speed/nlp.speed.noun')),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'china/nlp.china.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'aviation/nlp.aviation.noun',\n",
       "  ('+/gb',\n",
       "   \"'s/nlp.'s.part\",\n",
       "   ('+/gb',\n",
       "    'latest/nlp.late.adj',\n",
       "    ('+/gb', 'to/nlp.to.adp', 'temple/nlp.temple.noun')),\n",
       "   'china/nlp.china.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'norway/nlp.norway.noun', 'camp/nlp.camp.noun'),\n",
       "  ('+/gb',\n",
       "   'after/nlp.after.adp',\n",
       "   ('+/gb', 'four/nlp.four.num', 'years/nlp.year.noun'),\n",
       "   ('+/gb',\n",
       "    ('+/gb', 'mass/nlp.mass.noun', 'shooting/nlp.shooting.noun'),\n",
       "    'tragedy/nlp.tragedy.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'icc/nlp.icc.adj', 'prosecutor/nlp.prosecutor.noun'),\n",
       "  ('+/gb',\n",
       "   'second/nlp.second.adj',\n",
       "   ('+/gb',\n",
       "    'into/nlp.into.adp',\n",
       "    'investigation/nlp.investigation.noun',\n",
       "    ('+/gb',\n",
       "     'central/nlp.central.adj',\n",
       "     ('+/gb',\n",
       "      'african/nlp.african.adj',\n",
       "      ('+/gb',\n",
       "       'republic/nlp.republic.noun',\n",
       "       ('+/gb', 'war/nlp.war.noun', 'crimes/nlp.crime.noun'))))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'museum/nlp.museum.noun',\n",
       "   ('dedicated_to/nlp.dedicate.verb+nlp.to.adp',\n",
       "    ('+/gb',\n",
       "     'tiananmen/nlp.tiananmen.noun',\n",
       "     ('+/gb', 'square/nlp.square.adj', 'massacre/nlp.massacre.noun')))),\n",
       "  ('+/gb',\n",
       "   'in/nlp.in.adp',\n",
       "   ('+/gb', 'hong/nlp.hong.noun', 'kong/nlp.kong.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'ftc/nlp.ftc.noun',\n",
       "  ('+/gb',\n",
       "   'into/nlp.into.adp',\n",
       "   'inquiry/nlp.inquiry.noun',\n",
       "   'herbalife/nlp.herbalife.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'britain/nlp.britain.noun',\n",
       "  ('+/gb',\n",
       "   'egypt/nlp.egypt.adj',\n",
       "   ('+/gb',\n",
       "    'trade/nlp.trade.noun',\n",
       "    ('+/gb',\n",
       "     'largest/nlp.large.adj',\n",
       "     ('+/gb',\n",
       "      'over/nlp.over.adp',\n",
       "      'mission/nlp.mission.noun',\n",
       "      'in/nlp.in.adp',\n",
       "      ('+/gb', 'a/nlp.a.det', 'decade/nlp.decade.noun')))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   \"'s/nlp.'s.part\",\n",
       "   ('+/gb', 'glass/nlp.glass.noun', 'walkway/nlp.walkway.noun'),\n",
       "   'china/nlp.china.noun'),\n",
       "  ('+/gb',\n",
       "   'in/nlp.in.adp',\n",
       "   ('+/gb', 'tianmen/nlp.tianmen.noun', 'mountain/nlp.mountain.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'croatia/nlp.croatia.noun',\n",
       "  ('+/gb', 'serbia/nlp.serbia.noun', 'border/nlp.border.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'icc/nlp.icc.noun',\n",
       "  ('+/gb',\n",
       "   'into/nlp.into.adp',\n",
       "   'inquiry/nlp.inquiry.noun',\n",
       "   ('+/gb',\n",
       "    'possible/nlp.possible.adj',\n",
       "    ('+/gb',\n",
       "     'war/nlp.war.noun',\n",
       "     ('+/gb',\n",
       "      'in/nlp.in.adp',\n",
       "      'crimes/nlp.crime.noun',\n",
       "      ('+/gb',\n",
       "       'palestinian/nlp.palestinian.adj',\n",
       "       'territories/nlp.territory.noun')))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   \"'s/nlp.'s.part\",\n",
       "   ('+/gb',\n",
       "    'landmark/nlp.landmark.noun',\n",
       "    ('+/gb',\n",
       "     'wave/nlp.wave.noun',\n",
       "     ('+/gb', 'power/nlp.power.noun', 'station/nlp.station.noun'))),\n",
       "   'gibraltar/nlp.gibraltar.noun'),\n",
       "  ('+/gb', 'for/nlp.for.adp', 'business/nlp.business.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'uae/nlp.uae.noun',\n",
       "  ('+/gb',\n",
       "   'world/nlp.world.noun',\n",
       "   ('+/gb',\n",
       "    '’s/nlp.’s.adv',\n",
       "    ('+/gb',\n",
       "     'solar/nlp.solar.adj',\n",
       "     ('+/gb',\n",
       "      'power/nlp.power.noun',\n",
       "      ('+/gb', 'largest/nlp.large.adj', 'plant/nlp.plant.noun')))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', '5-star/nlp.5-star.adj', 'resort/nlp.resort.noun'),\n",
       "  ('+/gb',\n",
       "   'for/nlp.for.adp',\n",
       "   ('+/gb',\n",
       "    'isis/nlp.isi.noun',\n",
       "    ('+/gb',\n",
       "     'in/nlp.in.adp',\n",
       "     'supporters/nlp.supporter.noun',\n",
       "     'iraq/nlp.iraq.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   ('+/gb', 'china/nlp.china.noun', 'myanmar/nlp.myanmar.noun'),\n",
       "   'pipeline/nlp.pipeline.noun',\n",
       "   ('+/gb', 'crude/nlp.crude.adj', 'oil/nlp.oil.noun')),\n",
       "  ('+/gb',\n",
       "   'for/nlp.for.adp',\n",
       "   ('+/gb', 'test/nlp.test.noun', 'runs/nlp.run.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'singapore/nlp.singapore.noun',\n",
       "  ('+/gb',\n",
       "   'into/nlp.into.adp',\n",
       "   'inquiry/nlp.inquiry.noun',\n",
       "   'us/nlp.us.pron',\n",
       "   ('+/gb',\n",
       "    \"'s/nlp.'s.part\",\n",
       "    ('+/gb',\n",
       "     'for/nlp.for.adp',\n",
       "     ('+/gb',\n",
       "      ('+/gb', 'death/nlp.death.noun', 'family/nlp.family.noun'),\n",
       "      'report/nlp.report.noun',\n",
       "      ('+/gb', 'suicide/nlp.suicide.noun', 'autopsy/nlp.autopsy.adj'),\n",
       "      'dispute/nlp.dispute.noun'),\n",
       "     'scientist/nlp.scientist.noun'),\n",
       "    'man/nlp.man.noun'),\n",
       "   ('believed/nlp.believe.verb',\n",
       "    'who/nlp.who.noun',\n",
       "    ('was_endangering/nlp.be.verb+nlp.endanger.verb',\n",
       "     ('+/gb',\n",
       "      'us/nlp.us.pron',\n",
       "      ('+/gb', 'security/nlp.security.noun', 'he/nlp.he.pron')),\n",
       "     ('+/gb',\n",
       "      'by/nlp.by.adp',\n",
       "      ('working_with/nlp.work.verb+nlp.with.adp', 'china/nlp.china.noun')))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'russia/nlp.russia.noun',\n",
       "  ('+/gb',\n",
       "   'new/nlp.new.adj',\n",
       "   ('+/gb',\n",
       "    'in/nlp.in.adp',\n",
       "    'front/nlp.front.noun',\n",
       "    ('+/gb',\n",
       "     'rivalry/nlp.rivalry.noun',\n",
       "     ('+/gb', 'with/nlp.with.adp', 'u.s/nlp.u.s.propn'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'france/nlp.france.noun',\n",
       "  ('+/gb',\n",
       "   'against/nlp.against.adp',\n",
       "   ('+/gb',\n",
       "    ('+/gb', 'war/nlp.war.noun', 'crimes/nlp.crime.noun'),\n",
       "    'inquiry/nlp.inquiry.noun'),\n",
       "   ('+/gb', 'assad/nlp.assad.adj', 'regime/nlp.regime.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'europe/nlp.europe.noun',\n",
       "  ('+/gb',\n",
       "   'new/nlp.new.adj',\n",
       "   ('+/gb',\n",
       "    'cargo/nlp.cargo.noun',\n",
       "    ('+/gb',\n",
       "     'train/nlp.train.noun',\n",
       "     ('+/gb',\n",
       "      'between/nlp.between.adp',\n",
       "      'service/nlp.service.noun',\n",
       "      'china/nlp.china.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'china/nlp.china.noun',\n",
       "  ('+/gb',\n",
       "   'longest/nlp.long.adj',\n",
       "   ('+/gb',\n",
       "    'glass/nlp.glass.noun',\n",
       "    'bridge/nlp.bridge.noun',\n",
       "    'bottomed/nlp.bottom.verb'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'gezi/nlp.gezi.adj', 'protester/nlp.protester.noun'),\n",
       "  ('+/gb',\n",
       "   'legal/nlp.legal.adj',\n",
       "   'battle/nlp.battle.noun',\n",
       "   ('+/gb',\n",
       "    'with/nlp.with.adp',\n",
       "    ('showing/nlp.show.verb',\n",
       "     'footage/nlp.footage.noun',\n",
       "     'moment/nlp.moment.noun',\n",
       "     ('hits/nlp.hit.verb',\n",
       "      ('+/gb', 'tear/nlp.tear.noun', 'gas/nlp.ga.noun'),\n",
       "      'when/nlp.when.adv',\n",
       "      ('+/gb', 'him/nlp.him.pron', 'turkey/nlp.turkey.noun')))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'supermarket/nlp.supermarket.noun',\n",
       "   ('+/gb', 'the/nlp.the.det', ('+/gb', \"'s/nlp.'s.part\", 'uk/nlp.uk.propn')),\n",
       "   ('+/gb',\n",
       "    'first/nlp.first.adj',\n",
       "    ('+/gb', 'food/nlp.food.noun', 'waste/nlp.waste.noun'))),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'pictures/nlp.picture.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'iran/nlp.iran.noun',\n",
       "  ('+/gb',\n",
       "   'new/nlp.new.adj',\n",
       "   ('to/nlp.to.part',\n",
       "    'plant/nlp.plant.noun',\n",
       "    'counter/nlp.counter.verb',\n",
       "    'sanctions/nlp.sanction.noun'),\n",
       "   ('+/gb', 'gold/nlp.gold.noun', 'processing/nlp.processing.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'spectacular/nlp.spectacular.adj',\n",
       "   ('+/gb', 'water/nlp.water.noun', 'show/nlp.show.noun')),\n",
       "  ('+/gb', 'venice/nlp.venice.noun', 'carnival/nlp.carnival.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'venezuela/nlp.venezuela.noun',\n",
       "  ('+/gb',\n",
       "   'on/nlp.on.adp',\n",
       "   ('+/gb',\n",
       "    'for/nlp.for.adp',\n",
       "    'debate/nlp.debate.noun',\n",
       "    ('+/gb',\n",
       "     'alleged/nlp.allege.verb',\n",
       "     ('+/gb', 'political/nlp.political.adj', 'prisoners/nlp.prisoner.noun'))),\n",
       "   ('+/gb', 'amnesty/nlp.amnesty.noun', 'law/nlp.law.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'cafe/nlp.cafe.noun',\n",
       "   ('to_empower/nlp.to.part+nlp.empower.verb',\n",
       "    ('+/gb',\n",
       "     'the/nlp.the.det',\n",
       "     ('impaired/nlp.impair.verb', 'hearing/nlp.hearing.noun')))),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'indonesia/nlp.indonesia.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'japan/nlp.japan.noun',\n",
       "  ('+/gb',\n",
       "   'radar/nlp.radar.noun',\n",
       "   ('+/gb',\n",
       "    'close/nlp.close.adv',\n",
       "    'station/nlp.station.noun',\n",
       "    ('+/gb',\n",
       "     'to/nlp.to.adp',\n",
       "     ('+/gb',\n",
       "      'disputed/nlp.dispute.verb',\n",
       "      ('+/gb',\n",
       "       'east/nlp.east.adj',\n",
       "       ('+/gb',\n",
       "        'china/nlp.china.noun',\n",
       "        ('+/gb', 'sea/nlp.sea.noun', 'islands/nlp.island.noun')))))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'inspection/nlp.inspection.noun',\n",
       "   ('+/gb',\n",
       "    'team/nlp.team.noun',\n",
       "    ('+/gb',\n",
       "     'on/nlp.on.adp',\n",
       "     ('+/gb', 'chemical/nlp.chemical.adj', 'weapons/nlp.weapon.noun'),\n",
       "     'saturday/nlp.saturday.noun')),\n",
       "   ('+/gb', 'from/nlp.from.adp', 'syria/nlp.syria.adj')),\n",
       "  ('+/gb',\n",
       "   'a/nlp.a.det',\n",
       "   ('+/gb',\n",
       "    'for/nlp.for.adp',\n",
       "    'window/nlp.window.noun',\n",
       "    ('+/gb', 'a/nlp.a.det', 'u.s/nlp.u..noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'israel/nlp.israel.noun',\n",
       "  ('+/gb',\n",
       "   'dam/nlp.dam.adj',\n",
       "   ('+/gb', 'flood/nlp.flood.noun', 'gaza/nlp.gaza.noun'),\n",
       "   ('+/gb', \"'/nlp.'.part\", 'gates/nlp.gate.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'french/nlp.french.adj', 'prosecutor/nlp.prosecutor.noun'),\n",
       "  ('+/gb',\n",
       "   'into/nlp.into.adp',\n",
       "   'inquiry/nlp.inquiry.noun',\n",
       "   ('+/gb', 'macron/nlp.macron.noun', 'minister/nlp.minister.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'selfridges/nlp.selfridges.noun',\n",
       "  ('+/gb',\n",
       "   'its/nlp.its.adj',\n",
       "   ('+/gb',\n",
       "    'christmas/nlp.christmas.noun',\n",
       "    ('+/gb',\n",
       "     'before/nlp.before.adp',\n",
       "     'shop/nlp.shop.noun',\n",
       "     ('+/gb',\n",
       "      'a/nlp.a.det',\n",
       "      ('+/gb',\n",
       "       'mere/nlp.mere.adj',\n",
       "       ('+/gb', '143/nlp.143.num', 'days/nlp.day.noun'))),\n",
       "     'christmas/nlp.christmas.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'russia/nlp.russia.noun',\n",
       "  ('+/gb',\n",
       "   'case/nlp.case.noun',\n",
       "   ('+/gb',\n",
       "    'against/nlp.against.adp',\n",
       "    ('+/gb',\n",
       "     'artist/nlp.artist.noun',\n",
       "     ('nailed/nlp.nail.verb',\n",
       "      'who/nlp.who.noun',\n",
       "      ('+/gb', 'his/nlp.his.adj', 'testicles/nlp.testicle.noun'),\n",
       "      ('to_ground_in/nlp.to.part+nlp.grind.verb+nlp.in.adp',\n",
       "       ('+/gb', 'red/nlp.red.adj', 'square/nlp.square.noun'))))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'palestine/nlp.palestine.noun',\n",
       "  ('+/gb',\n",
       "   'its/nlp.its.adj',\n",
       "   ('+/gb',\n",
       "    'first/nlp.first.adj',\n",
       "    ('+/gb', 'national/nlp.national.adj', 'museum/nlp.museum.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'aids/nlp.aid.noun', 'orphanage/nlp.orphanage.noun'),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'myanmar/nlp.myanmar.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'taiwan/nlp.taiwan.det',\n",
       "   ('+/gb', 'cultural/nlp.cultural.adj', 'center/nlp.center.noun')),\n",
       "  ('+/gb',\n",
       "   'in/nlp.in.adp',\n",
       "   ('+/gb',\n",
       "    'new/nlp.new.adj',\n",
       "    ('+/gb', 'tokyo/nlp.tokyo.noun', 'location/nlp.location.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'israel/nlp.israel.noun',\n",
       "  ('+/gb',\n",
       "   'first/nlp.first.adv',\n",
       "   ('+/gb', 'nato/nlp.nato.adj', 'office/nlp.office.noun'),\n",
       "   'ever/nlp.ever.adv')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'the/nlp.the.det',\n",
       "   ('+/gb',\n",
       "    'first/nlp.first.adj',\n",
       "    ('+/gb',\n",
       "     'solar/nlp.solar.adj',\n",
       "     ('+/gb', 'panel/nlp.panel.noun', 'road/nlp.road.noun'))),\n",
       "   ('+/gb', \"'s/nlp.'s.part\", 'world/nlp.world.noun')),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'france/nlp.france.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'sinkhole/nlp.sinkhole.noun',\n",
       "  ('+/gb',\n",
       "   'under/nlp.under.adp',\n",
       "   ('+/gb',\n",
       "    'a/nlp.a.det',\n",
       "    ('+/gb',\n",
       "     'in/nlp.in.adp',\n",
       "     'house/nlp.house.noun',\n",
       "     'florida/nlp.florida.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   \"'s/nlp.'s.part\",\n",
       "   'oktoberfest/nlp.oktoberfest.noun',\n",
       "   'germany/nlp.germany.noun'),\n",
       "  ('+/gb',\n",
       "   'in/nlp.in.adp',\n",
       "   ('+/gb',\n",
       "    'of/nlp.of.adp',\n",
       "    'shadow/nlp.shadow.noun',\n",
       "    ('+/gb', 'refugee/nlp.refugee.noun', 'crisis/nlp.crisis.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'centres/nlp.centre.noun',\n",
       "  ('to_speed_vetting_to_be_set_up/nlp.to.part+nlp.speed.verb+nlp.to.part+nlp.vet.verb+nlp.set.verb+nlp.be.verb+nlp.up.part',\n",
       "   ('+/gb', 'screening/nlp.screening.noun', 'outposts/nlp.outpost.noun'),\n",
       "   ('+/gb', 'in/nlp.in.adp', 'iraq/nlp.iraq.noun'),\n",
       "   ('+/gb', 'as/nlp.as.adp', 'lebanon/nlp.lebanon.noun', 'u.s/nlp.u..noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   \"'s/nlp.'s.part\",\n",
       "   ('+/gb', 'dream/nlp.dream.noun', 'school/nlp.school.noun'),\n",
       "   'madonna/nlp.madonna.noun'),\n",
       "  ('+/gb',\n",
       "   'in/nlp.in.adp',\n",
       "   ('+/gb', 'karachi/nlp.karachi.noun', 'pakistan/nlp.pakistan.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'child/nlp.child.noun',\n",
       "   ('+/gb',\n",
       "    'sex/nlp.sex.noun',\n",
       "    ('+/gb', 'doll/nlp.doll.noun', 'trial/nlp.trial.noun'))),\n",
       "  ('+/gb',\n",
       "   \"'s/nlp.'s.part\",\n",
       "   ('+/gb',\n",
       "    'of/nlp.of.adp',\n",
       "    'box/nlp.box.noun',\n",
       "    ('+/gb',\n",
       "     'about/nlp.about.adp',\n",
       "     'questions/nlp.question.noun',\n",
       "     ('+/gb', 'child/nlp.child.noun', 'porn/nlp.porn.noun'))),\n",
       "   'pandora/nlp.pandora.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   \"'s/nlp.'s.part\",\n",
       "   ('+/gb',\n",
       "    'dog/nlp.dog.noun',\n",
       "    ('+/gb', 'meat/nlp.meat.noun', 'festival/nlp.festival.noun')),\n",
       "   'china/nlp.china.noun'),\n",
       "  ('+/gb',\n",
       "   'despite/nlp.despite.adp',\n",
       "   ('+/gb', 'ban/nlp.ban.noun', 'rumours/nlp.rumour.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'higher/nlp.high.adj',\n",
       "  ('+/gb',\n",
       "   'on/nlp.on.adp',\n",
       "   ('+/gb',\n",
       "    'of/nlp.of.adp',\n",
       "    'hopes/nlp.hope.noun',\n",
       "    ('+/gb',\n",
       "     'more/nlp.more.adj',\n",
       "     ('+/gb',\n",
       "      'in/nlp.in.adp',\n",
       "      'stimulus/nlp.stimulus.noun',\n",
       "      'china/nlp.china.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   \"'s/nlp.'s.part\",\n",
       "   ('+/gb',\n",
       "    'first/nlp.first.adj',\n",
       "    ('+/gb', 'western/nlp.western.adj', 'embassy/nlp.embassy.noun')),\n",
       "   'palestine/nlp.palestine.noun'),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'brazil/nlp.brazil.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   '’s/nlp.’s.part',\n",
       "   ('+/gb',\n",
       "    'first/nlp.first.adj',\n",
       "    ('+/gb',\n",
       "     'clean’/nlp.clean’.adj',\n",
       "     ('+/gb', 'coal/nlp.coal.noun', 'plant/nlp.plant.noun'))),\n",
       "   'world/nlp.world.noun',\n",
       "   ('+/gb', 'full/nlp.full.adj', 'scale/nlp.scale.noun')),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'canada/nlp.canada.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'largest/nlp.large.adj',\n",
       "   ('+/gb',\n",
       "    'concentrated/nlp.concentrated.adj',\n",
       "    ('+/gb',\n",
       "     'solar/nlp.solar.adj',\n",
       "     ('+/gb', 'power/nlp.power.noun', 'plant/nlp.plant.noun')))),\n",
       "  ('+/gb',\n",
       "   'in/nlp.in.adp',\n",
       "   ('+/gb', 'abu/nlp.abu.noun', 'dhabi/nlp.dhabi.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'a/nlp.a.det',\n",
       "   ('+/gb', 'new/nlp.new.adj', 'pipeline/nlp.pipeline.noun')),\n",
       "  ('+/gb', 'saudi/nlp.saudi.adj', 'arabia/nlp.arabia.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'china/nlp.china.noun',\n",
       "  ('+/gb',\n",
       "   'debt/nlp.debt.noun',\n",
       "   ('+/gb',\n",
       "    'for/nlp.for.adp',\n",
       "    'window/nlp.window.noun',\n",
       "    ('+/gb', 'local/nlp.local.adj', 'governments/nlp.government.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'world/nlp.world.noun',\n",
       "   ('’s/nlp.’s.adv',\n",
       "    ('+/gb',\n",
       "     'rail/nlp.rail.noun',\n",
       "     ('+/gb',\n",
       "      'and/nlp.and.conj',\n",
       "      'tunnel/nlp.tunnel.noun',\n",
       "      ('+/gb', 'deepest/nlp.deep.adj', 'longest/nlp.long.adj'))))),\n",
       "  ('+/gb',\n",
       "   'after/nlp.after.adp',\n",
       "   ('began/nlp.begin.verb',\n",
       "    ('+/gb',\n",
       "     'construction/nlp.construction.noun',\n",
       "     ('+/gb', '17/nlp.17.num', 'years/nlp.year.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'uruguay/nlp.uruguay.noun',\n",
       "  ('+/gb',\n",
       "   'in/nlp.in.adp',\n",
       "   'embassy/nlp.embassy.noun',\n",
       "   'palestine/nlp.palestine.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'interpol/nlp.interpol.noun',\n",
       "  ('+/gb',\n",
       "   'new/nlp.new.adj',\n",
       "   ('+/gb',\n",
       "    'in/nlp.in.adp',\n",
       "    ('+/gb',\n",
       "     'against/nlp.against.adp',\n",
       "     'front/nlp.front.noun',\n",
       "     'war/nlp.war.noun',\n",
       "     ('+/gb', 'wildlife/nlp.wildlife.noun', 'crimes/nlp.crime.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'first/nlp.first.adj',\n",
       "   ('+/gb', 'mushroom/nlp.mushroom.noun', 'farm/nlp.farm.noun')),\n",
       "  ('+/gb',\n",
       "   'in/nlp.in.adp',\n",
       "   ('+/gb',\n",
       "    'palestinian/nlp.palestinian.adj',\n",
       "    ('+/gb', 'west/nlp.west.adj', 'bank/nlp.bank.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'largest/nlp.large.adj',\n",
       "   ('+/gb', 'bid/nlp.bid.noun', 'trial/nlp.trial.noun'),\n",
       "   ('+/gb', 'post/nlp.post.noun', 'coup/nlp.coup.noun')),\n",
       "  ('+/gb',\n",
       "   'with/nlp.with.adp',\n",
       "   ('+/gb', '330/nlp.330.num', 'suspects/nlp.suspect.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'ethiopia/nlp.ethiopia.noun',\n",
       "  ('+/gb',\n",
       "   \"'s/nlp.'s.part\",\n",
       "   ('+/gb', 'biggest/nlp.big.adj', 'windfarm/nlp.windfarm.noun'),\n",
       "   'africa/nlp.africa.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'the/nlp.the.det',\n",
       "   ('+/gb',\n",
       "    'of/nlp.of.adp',\n",
       "    'prosecutor/nlp.prosecutor.noun',\n",
       "    ('+/gb',\n",
       "     'the/nlp.the.det',\n",
       "     ('+/gb',\n",
       "      'international/nlp.international.adj',\n",
       "      ('+/gb', 'criminal/nlp.criminal.adj', 'court/nlp.court.noun')))),\n",
       "   ('+/gb', 'fatou/nlp.fatou.noun', 'bensouda/nlp.bensouda.noun')),\n",
       "  ('+/gb',\n",
       "   'a/nlp.a.det',\n",
       "   ('+/gb',\n",
       "    'preliminary/nlp.preliminary.adj',\n",
       "    ('+/gb',\n",
       "     'of/nlp.of.adp',\n",
       "     'examination/nlp.examination.noun',\n",
       "     ('+/gb',\n",
       "      'the/nlp.the.det',\n",
       "      ('+/gb',\n",
       "       'in/nlp.in.adp',\n",
       "       'situation/nlp.situation.noun',\n",
       "       'palestine/nlp.palestine.noun')))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   \"'s/nlp.'s.part\",\n",
       "   ('+/gb', 'largest/nlp.large.adj', 'building/nlp.building.noun'),\n",
       "   'world/nlp.world.noun'),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'china/nlp.china.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'spain/nlp.spain.noun',\n",
       "  ('+/gb',\n",
       "   'biggest/nlp.big.adj',\n",
       "   ('+/gb', 'corruption/nlp.corruption.noun', 'trial/nlp.trial.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'russia/nlp.russia.noun',\n",
       "  ('+/gb',\n",
       "   'into/nlp.into.adp',\n",
       "   'inquiry/nlp.inquiry.noun',\n",
       "   ('+/gb',\n",
       "    'men/nlp.man.noun',\n",
       "    ('crushed/nlp.crush.verb',\n",
       "     'who/nlp.who.noun',\n",
       "     'bear/nlp.bear.noun',\n",
       "     ('+/gb', 'to/nlp.to.adp', 'death/nlp.death.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'islamic/nlp.islamic.adj', 'state/nlp.state.noun'),\n",
       "  ('+/gb',\n",
       "   'new/nlp.new.adj',\n",
       "   ('+/gb',\n",
       "    'in/nlp.in.adp',\n",
       "    'front/nlp.front.noun',\n",
       "    ('+/gb', 'north/nlp.north.noun', 'africa/nlp.africa.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'netherlands/nlp.netherlands.noun',\n",
       "  ('+/gb',\n",
       "   'into/nlp.into.adp',\n",
       "   ('+/gb',\n",
       "    ('+/gb', 'war/nlp.war.noun', 'crimes/nlp.crime.noun'),\n",
       "    'investigation/nlp.investigation.noun'),\n",
       "   ('+/gb',\n",
       "    'mh17/nlp.mh17.adj',\n",
       "    ('+/gb', 'airliner/nlp.airliner.noun', 'downing/nlp.downing.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'germany/nlp.germany.noun',\n",
       "  ('+/gb',\n",
       "   'investigation/nlp.investigation.noun',\n",
       "   ('+/gb',\n",
       "    'into/nlp.into.adp',\n",
       "    ('+/gb',\n",
       "     'turkish/nlp.turkish.adj',\n",
       "     ('accused/nlp.accuse.verb',\n",
       "      'group/nlp.group.noun',\n",
       "      ('of_spying_on/nlp.of.adp+nlp.spy.verb+nlp.on.adp',\n",
       "       ('+/gb',\n",
       "        'erdogan/nlp.erdogan.adj',\n",
       "        'opponents/nlp.opponent.noun',\n",
       "        ('+/gb',\n",
       "         'in/nlp.in.adp',\n",
       "         ('+/gb', '35/nlp.35.num', 'countries/nlp.country.noun'))))))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'senate/nlp.senate.adj', 'panel/nlp.panel.noun'),\n",
       "  ('+/gb',\n",
       "   'on/nlp.on.adp',\n",
       "   ('+/gb',\n",
       "    'into/nlp.into.adp',\n",
       "    'investigation/nlp.investigation.noun',\n",
       "    ('+/gb',\n",
       "     'corrupution/nlp.corrupution.noun',\n",
       "     ('+/gb',\n",
       "      'in/nlp.in.adp',\n",
       "      'probe/nlp.probe.noun',\n",
       "      'bangladesh/nlp.bangladesh.noun'))),\n",
       "   ('+/gb',\n",
       "    'of/nlp.of.adp',\n",
       "    'secretary/nlp.secretary.noun',\n",
       "    ('+/gb', 'state/nlp.state.noun', 'meddling/nlp.meddling.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'nasa/nlp.nasa.noun',\n",
       "  ('+/gb',\n",
       "   'media/nlp.medium.noun',\n",
       "   ('+/gb',\n",
       "    'for/nlp.for.adp',\n",
       "    'accreditation/nlp.accreditation.noun',\n",
       "    ('+/gb',\n",
       "     'next/nlp.next.adj',\n",
       "     ('+/gb',\n",
       "      ('+/gb',\n",
       "       'space/nlp.space.noun',\n",
       "       ('+/gb',\n",
       "        ('+/gb', 'orbital/nlp.orbital.adj', 'sciences/nlp.science.noun'),\n",
       "        'station/nlp.station.noun')),\n",
       "      'launch/nlp.launch.noun',\n",
       "      'resupply/nlp.resupply.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'new/nlp.new.adj',\n",
       "   ('+/gb', 'railway/nlp.railway.noun', 'link/nlp.link.noun')),\n",
       "  ('+/gb',\n",
       "   'between/nlp.between.adp',\n",
       "   ('+/gb', 'china/nlp.china.noun', 'russia/nlp.russia.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'controversial/nlp.controversial.adj',\n",
       "   ('+/gb', 'immigration/nlp.immigration.noun', 'court/nlp.court.noun')),\n",
       "  ('+/gb',\n",
       "   'at/nlp.at.adp',\n",
       "   ('+/gb', 'paris/nlp.paris.adj', 'airport/nlp.airport.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'france/nlp.france.noun',\n",
       "  ('+/gb',\n",
       "   'of/nlp.of.adp',\n",
       "   ('+/gb',\n",
       "    ('+/gb', 'war/nlp.war.noun', 'crimes/nlp.crime.noun'),\n",
       "    'probe/nlp.probe.noun'),\n",
       "   ('+/gb', 'syrian/nlp.syrian.adj', 'regime/nlp.regime.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'google/nlp.google.noun',\n",
       "  ('+/gb',\n",
       "   'its/nlp.its.adj',\n",
       "   ('first/nlp.first.adj',\n",
       "    ('+/gb', 'store/nlp.store.noun', 'opened/nlp.open.verb')),\n",
       "   ('+/gb',\n",
       "    'the/nlp.the.det',\n",
       "    ('+/gb', 'tech/nlp.tech.noun', 'giant/nlp.giant.noun')),\n",
       "   ('+/gb',\n",
       "    'its/nlp.its.adj',\n",
       "    ('ever/nlp.ever.adv',\n",
       "     ('+/gb',\n",
       "      'and/nlp.and.conj',\n",
       "      ('+/gb',\n",
       "       'mortar/nlp.mortar.noun',\n",
       "       ('+/gb', 'first/nlp.first.adj', 'store/nlp.store.noun'),\n",
       "       'brick/nlp.brick.noun')))),\n",
       "   ('+/gb', 'in/nlp.in.adp', 'london/nlp.london.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'general/nlp.general.adj', 'assembly/nlp.assembly.noun'),\n",
       "  ('+/gb',\n",
       "   'with/nlp.with.adp',\n",
       "   ('+/gb',\n",
       "    'on/nlp.on.adp',\n",
       "    'focus/nlp.focus.noun',\n",
       "    ('+/gb',\n",
       "     'and/nlp.and.conj',\n",
       "     'syria/nlp.syria.noun',\n",
       "     'isis/nlp.isi.noun',\n",
       "     'refugees/nlp.refugee.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   \"'s/nlp.'s.part\",\n",
       "   ('second/nlp.second.adv',\n",
       "    ('+/gb',\n",
       "     'offshore/nlp.offshore.adj',\n",
       "     ('+/gb',\n",
       "      'wind/nlp.wind.noun',\n",
       "      ('+/gb', 'largest/nlp.large.adj', 'farm/nlp.farm.noun')))),\n",
       "   'world/nlp.world.noun'),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'uk/nlp.uk.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'jakarta/nlp.jakarta.adj', 'airport/nlp.airport.noun'),\n",
       "  ('+/gb',\n",
       "   '£/nlp.£.sym',\n",
       "   'terminal/nlp.terminal.noun',\n",
       "   'million/nlp.million.num',\n",
       "   '430/nlp.430.num',\n",
       "   'm/nlp.m.num')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'pakistan/nlp.pakistan.noun',\n",
       "  ('+/gb',\n",
       "   'trade/nlp.trade.noun',\n",
       "   ('linking/nlp.link.verb',\n",
       "    'road/nlp.road.noun',\n",
       "    'afghanistan/nlp.afghanistan.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'julian/nlp.julian.adj',\n",
       "   ('+/gb', 'wikileaks/nlp.wikileaks.adj', 'party/nlp.party.noun'),\n",
       "   ('+/gb', \"'s/nlp.'s.part\", 'assange/nlp.assange.noun')),\n",
       "  ('+/gb',\n",
       "   'for/nlp.for.adp',\n",
       "   ('+/gb',\n",
       "    'in/nlp.in.adp',\n",
       "    'membership/nlp.membership.noun',\n",
       "    'australia/nlp.australia.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'first/nlp.first.adj',\n",
       "   ('+/gb', 'fuel/nlp.fuel.noun', 'station/nlp.station.noun'),\n",
       "   ('+/gb', 'public/nlp.public.adj', 'hydrogen/nlp.hydrogen.noun')),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'flanders/nlp.flanders.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'france/nlp.france.noun',\n",
       "  ('+/gb',\n",
       "   ('+/gb', 'iraq/nlp.iraq.adj', 'conference/nlp.conference.noun'),\n",
       "   ('urging/nlp.urge.verb',\n",
       "    ('+/gb',\n",
       "     'global/nlp.global.adj',\n",
       "     'fight/nlp.fight.noun',\n",
       "     ('+/gb', 'on/nlp.on.adp', 'jihadists/nlp.jihadist.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'ashton/nlp.ashton.noun', 'carter/nlp.carter.noun'),\n",
       "  ('+/gb', 'to/nlp.to.adp', 'door/nlp.door.noun', 'u.s/nlp.u..noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'new/nlp.new.adj',\n",
       "   ('+/gb', 'brics/nlp.brics.noun', 'bank/nlp.bank.noun')),\n",
       "  ('+/gb',\n",
       "   'for/nlp.for.adp',\n",
       "   ('+/gb', 'xinhua/nlp.xinhua.noun', 'business/nlp.business.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'first/nlp.first.adj',\n",
       "   ('+/gb',\n",
       "    'palestinian/nlp.palestinian.adj',\n",
       "    ('+/gb',\n",
       "     'in/nlp.in.adp',\n",
       "     'embassy/nlp.embassy.noun',\n",
       "     ('+/gb', 'w./nlp.w..propn', 'europe/nlp.europe.noun')))),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'sweden/nlp.sweden.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'in/nlp.in.adp',\n",
       "   'museum/nlp.museum.noun',\n",
       "   'germany/nlp.germany.noun'),\n",
       "  ('+/gb',\n",
       "   'floating/nlp.float.verb',\n",
       "   ('+/gb',\n",
       "    'playground/nlp.playground.noun',\n",
       "    ('+/gb', 'space/nlp.space.noun', 'inspired/nlp.inspire.verb')),\n",
       "   ('puts/nlp.put.verb',\n",
       "    'that/nlp.that.adp',\n",
       "    'visitors/nlp.visitor.noun',\n",
       "    ('+/gb', 'into/nlp.into.adp', 'orbit/nlp.orbit.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'canadian/nlp.canadian.adj',\n",
       "   'centre/nlp.centre.noun',\n",
       "   ('+/gb', 'refugee/nlp.refugee.noun', 'processing/nlp.process.verb')),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'jordan/nlp.jordan.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'chinese/nlp.chinese.adj',\n",
       "   ('+/gb', 'carbon/nlp.carbon.noun', 'market/nlp.market.noun')),\n",
       "  ('+/gb',\n",
       "   'to/nlp.to.adp',\n",
       "   ('+/gb',\n",
       "    'a/nlp.a.det',\n",
       "    ('+/gb',\n",
       "     'busy/nlp.busy.adj',\n",
       "     ('+/gb', 'first/nlp.first.adj', 'day/nlp.day.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'bolivia/nlp.bolivia.noun',\n",
       "  ('+/gb',\n",
       "   'anti/nlp.anti.adj',\n",
       "   ('+/gb',\n",
       "    'imperialist/nlp.imperialist.adj',\n",
       "    ('to/nlp.to.part',\n",
       "     'school/nlp.school.noun',\n",
       "     'counter/nlp.counter.verb',\n",
       "     'us/nlp.us.pron')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   \"'s/nlp.'s.part\",\n",
       "   ('+/gb',\n",
       "    'longest/nlp.long.adj',\n",
       "    ('+/gb',\n",
       "     'rail/nlp.rail.noun',\n",
       "     'tunnel/nlp.tunnel.noun',\n",
       "     ('+/gb', 'deepest/nlp.deep.adj', 'and/nlp.and.conj'))),\n",
       "   'world/nlp.world.noun'),\n",
       "  ('+/gb',\n",
       "   'through/nlp.through.adp',\n",
       "   ('+/gb', 'swiss/nlp.swiss.adj', 'alps/nlp.alp.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'palestinian/nlp.palestinian.adj', 'embassy/nlp.embassy.noun'),\n",
       "  ('+/gb',\n",
       "   'in/nlp.in.adp',\n",
       "   ('+/gb', 'vatican/nlp.vatican.adj', 'city/nlp.city.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'china/nlp.china.noun',\n",
       "  ('+/gb',\n",
       "   'terrifying/nlp.terrifying.adj',\n",
       "   ('+/gb',\n",
       "    'glass/nlp.glass.noun',\n",
       "    ('+/gb',\n",
       "     'in/nlp.in.adp',\n",
       "     'bridge/nlp.bridge.noun',\n",
       "     'bottomed/nlp.bottom.verb',\n",
       "     ('+/gb', 'shiniuzhai/nlp.shiniuzhai.noun', 'park/nlp.park.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   \"'s/nlp.'s.part\",\n",
       "   ('+/gb', 'dalian/nlp.dalian.noun', 'wanda/nlp.wanda.noun'),\n",
       "   'china/nlp.china.noun'),\n",
       "  ('+/gb',\n",
       "   '$/nlp.$.sym',\n",
       "   ('+/gb', 'tourism/nlp.tourism.noun', 'park/nlp.park.noun'),\n",
       "   'billion/nlp.billion.num',\n",
       "   '5.1/nlp.5.1.num')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'migrant/nlp.migrant.noun',\n",
       "   ('made/nlp.make.verb',\n",
       "    'shelter/nlp.shelter.noun',\n",
       "    ('of/nlp.of.adp',\n",
       "     ('+/gb',\n",
       "      'shipping/nlp.shipping.noun',\n",
       "      'containers/nlp.container.noun')))),\n",
       "  ('+/gb',\n",
       "   'in/nlp.in.adp',\n",
       "   ('+/gb',\n",
       "    \"'s/nlp.'s.part\",\n",
       "    'calais/nlp.calais.noun',\n",
       "    'france/nlp.france.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'iraq/nlp.iraq.noun',\n",
       "  ('+/gb',\n",
       "   'new/nlp.new.adj',\n",
       "   ('+/gb',\n",
       "    'tigris/nlp.tigris.noun',\n",
       "    ('+/gb', 'bridge/nlp.bridge.noun', 'escape/nlp.escape.verb')),\n",
       "   ('+/gb',\n",
       "    'for/nlp.for.adp',\n",
       "    'route/nlp.route.noun',\n",
       "    ('+/gb',\n",
       "     'people/nlp.people.noun',\n",
       "     ('fleeing/nlp.flee.verb', 'mosul/nlp.mosul.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'dod/nlp.dod.noun',\n",
       "  ('+/gb',\n",
       "   'afghan/nlp.afghan.adp',\n",
       "   'probe/nlp.probe.noun',\n",
       "   ('+/gb',\n",
       "    'child/nlp.child.noun',\n",
       "    ('+/gb', 'sex/nlp.sex.noun', 'abuse/nlp.abuse.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'arab/nlp.arab.adj',\n",
       "   ('+/gb', 'league/nlp.league.noun', 'summit/nlp.summit.noun')),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'doha/nlp.doha.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'uk/nlp.uk.noun',\n",
       "  ('+/gb',\n",
       "   'into/nlp.into.adp',\n",
       "   'inquiry/nlp.inquiry.noun',\n",
       "   ('+/gb',\n",
       "    ('+/gb',\n",
       "     'russian/nlp.russian.adj',\n",
       "     ('+/gb',\n",
       "      'spy/nlp.spy.noun',\n",
       "      ('+/gb', \"'s/nlp.'s.part\", 'litvinenko/nlp.litvinenko.noun'))),\n",
       "    'death/nlp.death.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'chinese/nlp.chinese.adj', 'city/nlp.city.noun'),\n",
       "  ('+/gb',\n",
       "   'phone/nlp.phone.noun',\n",
       "   ('+/gb',\n",
       "    ('+/gb', 'for/nlp.for.adp', 'lane/nlp.lane.noun'),\n",
       "    ('texting/nlp.texting.verb', 'pedestrians/nlp.pedestrian.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'amsterdam/nlp.amsterdam.adj', 'mayor/nlp.mayor.noun'),\n",
       "  ('+/gb',\n",
       "   'brothel/nlp.brothel.noun',\n",
       "   'run/nlp.run.verb',\n",
       "   ('+/gb', 'by/nlp.by.adp', 'prostitutes/nlp.prostitute.noun'),\n",
       "   (\"'s/nlp.'.verb\",\n",
       "    'it/nlp.it.pron',\n",
       "    ('+/gb',\n",
       "     'a/nlp.a.det',\n",
       "     ('+/gb',\n",
       "      'whole/nlp.whole.adj',\n",
       "      ('+/gb', 'new/nlp.new.adj', 'model/nlp.model.noun')))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'hariri/nlp.hariri.adj',\n",
       "   ('+/gb', 'murder/nlp.murder.noun', 'trial/nlp.trial.noun')),\n",
       "  ('+/gb',\n",
       "   'in/nlp.in.adp',\n",
       "   ('+/gb', 'the/nlp.the.det', 'hague/nlp.hague.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   \"'s/nlp.'s.part\",\n",
       "   'aggression/nlp.aggression.noun',\n",
       "   'israel/nlp.israel.noun'),\n",
       "  ('+/gb',\n",
       "   'door/nlp.door.noun',\n",
       "   ('+/gb',\n",
       "    'to/nlp.to.adp',\n",
       "    ('+/gb',\n",
       "     'syrian/nlp.syrian.adj',\n",
       "     ('+/gb',\n",
       "      'information/nlp.information.noun',\n",
       "      'minister/nlp.minister.noun'),\n",
       "     ('+/gb', 'all/nlp.all.det', 'possibilities/nlp.possibility.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'saudi/nlp.saudi.adj', 'arabia/nlp.arabia.noun'),\n",
       "  'luxury/nlp.luxury.noun'),\n",
       " ('opens/nlp.open.verb',\n",
       "  'nato/nlp.nato.noun',\n",
       "  ('+/gb',\n",
       "   'regional/nlp.regional.adj',\n",
       "   ('+/gb',\n",
       "    'in/nlp.in.adp',\n",
       "    'center/nlp.center.noun',\n",
       "    'kuwait/nlp.kuwait.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'european/nlp.european.adj', 'commission/nlp.commission.noun'),\n",
       "  ('+/gb',\n",
       "   'antitrust/nlp.antitrust.adj',\n",
       "   ('+/gb',\n",
       "    'into/nlp.into.adp',\n",
       "    'investigation/nlp.investigation.noun',\n",
       "    ('+/gb',\n",
       "     \"'s/nlp.'s.part\",\n",
       "     'business/nlp.business.noun',\n",
       "     'amazon/nlp.amazon.noun',\n",
       "     ('+/gb', 'e/nlp.e.noun', 'book/nlp.book.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'chile/nlp.chile.noun',\n",
       "  ('+/gb',\n",
       "   'largest/nlp.large.adj',\n",
       "   ('+/gb',\n",
       "    ('+/gb',\n",
       "     'latin/nlp.latin.adj',\n",
       "     ('+/gb', \"'s/nlp.'s.part\", 'america/nlp.america.noun')),\n",
       "    'farm/nlp.farm.noun'),\n",
       "   ('+/gb', 'medical/nlp.medical.adj', 'marijuana/nlp.marijuana.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'nazi/nlp.nazi.adj',\n",
       "   ('+/gb',\n",
       "    'museum/nlp.museum.noun',\n",
       "    ('long/nlp.long.adv', 'delayed/nlp.delay.verb'))),\n",
       "  ('+/gb',\n",
       "   'in/nlp.in.adp',\n",
       "   ('+/gb',\n",
       "    'of/nlp.of.adp',\n",
       "    'home/nlp.home.noun',\n",
       "    ('+/gb', 'the/nlp.the.det', 'movement/nlp.movement.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'paris/nlp.paris.noun',\n",
       "  ('+/gb',\n",
       "   'monitored/nlp.monitored.adj',\n",
       "   ('+/gb',\n",
       "    'in/nlp.in.adp',\n",
       "    'gallery/nlp.gallery.noun',\n",
       "    ('+/gb', 'tourist/nlp.tourist.noun', 'location/nlp.location.noun')),\n",
       "   ('+/gb', 'drug/nlp.drug.noun', 'shooting/nlp.shooting.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'chinese/nlp.chinese.adj',\n",
       "   ('+/gb', 'cargo/nlp.cargo.noun', 'ship/nlp.ship.noun')),\n",
       "  ('+/gb',\n",
       "   'new/nlp.new.adj',\n",
       "   ('+/gb',\n",
       "    'trade/nlp.trade.noun',\n",
       "    ('+/gb',\n",
       "     'to/nlp.to.adp',\n",
       "     'route/nlp.route.noun',\n",
       "     'europe/nlp.europe.verb')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'catalonia/nlp.catalonia.noun',\n",
       "  ('+/gb',\n",
       "   'its/nlp.its.adj',\n",
       "   ('+/gb', 'own/nlp.own.adj', 'agency/nlp.agency.noun'),\n",
       "   ('+/gb', 'tax/nlp.tax.noun', 'collecting/nlp.collecting.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'france/nlp.france.noun',\n",
       "  ('+/gb',\n",
       "   'scandal/nlp.scandal.noun',\n",
       "   ('+/gb',\n",
       "    ('+/gb', 'volkswagen/nlp.volkswagen.noun', 'emissions/nlp.emission.noun'),\n",
       "    'probe/nlp.probe.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'gaza/nlp.gaza.noun',\n",
       "   ('+/gb',\n",
       "    'reconstruction/nlp.reconstruction.noun',\n",
       "    'conference/nlp.conference.noun')),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'cairo/nlp.cairo.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'dalai/nlp.dalai.noun', 'lama/nlp.lama.noun'),\n",
       "  ('+/gb',\n",
       "   'international/nlp.international.adj',\n",
       "   ('+/gb', 'buddhist/nlp.buddhist.adj', 'conference/nlp.conference.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'the/nlp.the.det', 'philippines/nlp.philippine.noun'),\n",
       "  ('+/gb',\n",
       "   'its/nlp.its.adj',\n",
       "   ('+/gb',\n",
       "    'first/nlp.first.adj',\n",
       "    ('+/gb',\n",
       "     'for/nlp.for.adp',\n",
       "     'school/nlp.school.noun',\n",
       "     ('+/gb',\n",
       "      'e/nlp.e.noun',\n",
       "      ('+/gb',\n",
       "       'sports/nlp.sport.noun',\n",
       "       'professionals/nlp.professional.noun')))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   '1st/nlp.1st.adj',\n",
       "   ('+/gb',\n",
       "    'transgender/nlp.transgender.noun',\n",
       "    ('+/gb', 'hair/nlp.hair.noun', 'salon/nlp.salon.noun'))),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'argentina/nlp.argentina.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'polar/nlp.polar.adj', 'thaw/nlp.thaw.noun'),\n",
       "  ('+/gb',\n",
       "   'for/nlp.for.adp',\n",
       "   'shortcut/nlp.shortcut.noun',\n",
       "   ('+/gb',\n",
       "    'russian/nlp.russian.adj',\n",
       "    ('+/gb', 'natural/nlp.natural.adj', 'gas/nlp.ga.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   \"'s/nlp.'s.part\",\n",
       "   ('+/gb',\n",
       "    'smallest/nlp.small.adj',\n",
       "    ('+/gb',\n",
       "     ('+/gb', 'mobile/nlp.mobile.adj', 'phone/nlp.phone.noun'),\n",
       "     'shop/nlp.shop.noun',\n",
       "     'repair/nlp.repair.noun')),\n",
       "   'world/nlp.world.noun'),\n",
       "  ('+/gb',\n",
       "   'inside/nlp.inside.adp',\n",
       "   ('+/gb',\n",
       "    'iconic/nlp.iconic.adj',\n",
       "    ('+/gb',\n",
       "     'red/nlp.red.adj',\n",
       "     ('+/gb', 'telephone/nlp.telephone.noun', 'box/nlp.box.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   \"'s/nlp.'s.part\",\n",
       "   ('+/gb', 'first/nlp.first.adj', 'mosque/nlp.mosque.noun'),\n",
       "   'denmark/nlp.denmark.noun'),\n",
       "  ('+/gb', 'amid/nlp.amid.adp', 'controversy/nlp.controversy.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'icelandic/nlp.icelandic.adj',\n",
       "   ('+/gb', 'drilling/nlp.drilling.noun', 'project/nlp.project.noun')),\n",
       "  ('+/gb',\n",
       "   'to/nlp.to.adp',\n",
       "   'door/nlp.door.noun',\n",
       "   ('+/gb',\n",
       "    'electricity/nlp.electricity.noun',\n",
       "    ('+/gb', 'volcano/nlp.volcano.noun', 'powered/nlp.power.verb')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'egypt/nlp.egypt.noun',\n",
       "  ('+/gb',\n",
       "   'new/nlp.new.adj',\n",
       "   ('+/gb', 'suez/nlp.suez.noun', 'canal/nlp.canal.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'honda/nlp.honda.noun',\n",
       "  ('+/gb',\n",
       "   'self/nlp.self.noun',\n",
       "   ('+/gb',\n",
       "    'hydrogen/nlp.hydrogen.noun',\n",
       "    ('+/gb', 'refuelling/nlp.refuelling.noun', 'station/nlp.station.noun')),\n",
       "   'sufficient/nlp.sufficient.adj')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'turkey/nlp.turkey.noun',\n",
       "  ('+/gb',\n",
       "   'tunnel/nlp.tunnel.noun',\n",
       "   ('linking/nlp.link.verb',\n",
       "    ('+/gb',\n",
       "     'and/nlp.and.conj',\n",
       "     ('+/gb', 'asia/nlp.asia.noun', 'europe/nlp.europe.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'hague/nlp.hague.noun', 'court/nlp.court.noun'),\n",
       "  ('+/gb',\n",
       "   'at/nlp.at.adp',\n",
       "   ('+/gb',\n",
       "    ('+/gb', 'war/nlp.war.noun', 'crimes/nlp.crime.noun'),\n",
       "    'inquiry/nlp.inquiry.noun'),\n",
       "   ('+/gb',\n",
       "    'of/nlp.of.adp',\n",
       "    'request/nlp.request.noun',\n",
       "    'palestinians/nlp.palestinian.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'volcanic/nlp.volcanic.adj',\n",
       "   ('+/gb', 'sleeping/nlp.sleeping.noun', 'giant/nlp.giant.noun')),\n",
       "  ('+/gb',\n",
       "   'north/nlp.north.adj',\n",
       "   ('+/gb', 'co/nlp.co.noun', 'operation/nlp.operation.noun'),\n",
       "   'korean/nlp.korean.adj')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'giant/nlp.giant.adj', 'aquarium/nlp.aquarium.noun'),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'mexico/nlp.mexico.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'turkey/nlp.turkey.noun',\n",
       "  ('+/gb',\n",
       "   'first/nlp.first.adj',\n",
       "   ('+/gb',\n",
       "    'against/nlp.against.adp',\n",
       "    'case/nlp.case.noun',\n",
       "    ('+/gb',\n",
       "     'oil/nlp.oil.noun',\n",
       "     ('+/gb',\n",
       "      'with/nlp.with.adp',\n",
       "      'smuggling/nlp.smuggle.verb',\n",
       "      'syria/nlp.syria.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'india/nlp.india.noun',\n",
       "  ('+/gb', 'bitcoin/nlp.bitcoin.noun', 'exchange/nlp.exchange.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'russia/nlp.russia.noun',\n",
       "  ('+/gb',\n",
       "   'of/nlp.of.adp',\n",
       "   ('+/gb', 'black/nlp.black.adj', 'box/nlp.box.noun'),\n",
       "   ('+/gb',\n",
       "    'jet/nlp.jet.noun',\n",
       "    ('downed_by/nlp.down.verb+nlp.by.adp', 'turkey/nlp.turkey.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'germany/nlp.germany.noun',\n",
       "  ('+/gb',\n",
       "   'of/nlp.of.adp',\n",
       "   'investigation/nlp.investigation.noun',\n",
       "   ('+/gb',\n",
       "    ('+/gb', \"'s/nlp.'s.part\", 'facebook/nlp.facebook.noun'),\n",
       "    'practices/nlp.practice.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'russia/nlp.russia.noun',\n",
       "  ('+/gb',\n",
       "   'new/nlp.new.adj',\n",
       "   ('+/gb',\n",
       "    'criminal/nlp.criminal.adj',\n",
       "    ('involving/nlp.involve.verb',\n",
       "     'case/nlp.case.noun',\n",
       "     ('+/gb',\n",
       "      'putin/nlp.putin.adj',\n",
       "      ('+/gb', 'critic/nlp.critic.noun', 'navalny/nlp.navalny.noun')))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'of/nlp.of.adp',\n",
       "   'trial/nlp.trial.noun',\n",
       "   'picasso/nlp.picasso.noun',\n",
       "   ('+/gb',\n",
       "    '’s/nlp.’s.num',\n",
       "    ('+/gb', 'ex/nlp.ex.noun', 'electrician/nlp.electrician.noun'),\n",
       "    ('accused_of_stealing/nlp.accuse.verb+nlp.of.adp+nlp.steal.verb',\n",
       "     'artworks/nlp.artwork.noun'))),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'france/nlp.france.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'bolivia/nlp.bolivia.noun',\n",
       "  ('+/gb',\n",
       "   'anti/nlp.anti.adj',\n",
       "   ('+/gb', 'military/nlp.military.adj', 'school/nlp.school.noun'),\n",
       "   'imperialist/nlp.imperialist.adj')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'germany/nlp.germany.noun',\n",
       "  ('+/gb',\n",
       "   'investigation/nlp.investigation.noun',\n",
       "   ('+/gb',\n",
       "    'into/nlp.into.adp',\n",
       "    ('+/gb',\n",
       "     'alleged/nlp.alleged.adj',\n",
       "     ('+/gb', 'nsa/nlp.nsa.propn', 'tap/nlp.tap.noun'),\n",
       "     ('+/gb', 'merkel/nlp.merkel.noun', 'phone/nlp.phone.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'putin/nlp.putin.noun',\n",
       "   ('+/gb', 'themed/nlp.themed.adj', 'pub/nlp.pub.noun')),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'kyrgyzstan/nlp.kyrgyzstan.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'uk/nlp.uk.intj',\n",
       "  ('+/gb',\n",
       "   'secret/nlp.secret.adj',\n",
       "   ('+/gb',\n",
       "    'about/nlp.about.adp',\n",
       "    'files/nlp.file.noun',\n",
       "    ('+/gb',\n",
       "     'jewish/nlp.jewish.adj',\n",
       "     'terrorists/nlp.terrorist.noun',\n",
       "     ('+/gb', 'in/nlp.in.adp', '1940s/nlp.1940.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'trial/nlp.trial.noun',\n",
       "  ('+/gb',\n",
       "   'for/nlp.for.adp',\n",
       "   ('+/gb',\n",
       "    '49/nlp.49.num',\n",
       "    ('accused_in/nlp.accuse.verb+nlp.in.adp',\n",
       "     ('+/gb',\n",
       "      'fatal/nlp.fatal.adj',\n",
       "      ('+/gb',\n",
       "       ('+/gb',\n",
       "        'afghan/nlp.afghan.adj',\n",
       "        ('+/gb', \"'s/nlp.'s.part\", 'woman/nlp.woman.noun')),\n",
       "       'beating/nlp.beating.noun')))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'cuban/nlp.cuban.adj', 'parliament/nlp.parliament.noun'),\n",
       "  ('+/gb', '2-day/nlp.2-day.num', 'session/nlp.session.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'milan/nlp.milan.noun', 'prosecutor/nlp.prosecutor.noun'),\n",
       "  ('+/gb',\n",
       "   'into/nlp.into.adp',\n",
       "   'investigation/nlp.investigation.noun',\n",
       "   ('+/gb', 'hacking/nlp.hacking.noun', 'team/nlp.team.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   '’s/nlp.’s.part',\n",
       "   ('+/gb',\n",
       "    '1st/nlp.1st.adj',\n",
       "    ('+/gb', 'private/nlp.private.adj', 'site/nlp.site.noun')),\n",
       "   'world/nlp.world.noun',\n",
       "   ('+/gb', 'space/nlp.space.noun', 'launch/nlp.launch.noun')),\n",
       "  ('+/gb',\n",
       "   'in/nlp.in.adp',\n",
       "   ('+/gb', 'new/nlp.new.adj', 'zealand/nlp.zealand.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'russia/nlp.russia.noun',\n",
       "  ('+/gb',\n",
       "   'criminal/nlp.criminal.adj',\n",
       "   ('+/gb',\n",
       "    'on/nlp.on.adp',\n",
       "    ('+/gb',\n",
       "     'for/nlp.for.adp',\n",
       "     'case/nlp.case.noun',\n",
       "     'kiev/nlp.kiev.noun',\n",
       "     ('+/gb',\n",
       "      'in/nlp.in.adp',\n",
       "      'genocide/nlp.genocide.noun',\n",
       "      'east/nlp.east.noun'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'conservative/nlp.conservative.adj',\n",
       "   ('+/gb', 'leadership/nlp.leadership.noun', 'rift/nlp.rift.noun')),\n",
       "  ('+/gb',\n",
       "   'as/nlp.as.adp',\n",
       "   'begin/nlp.begin.verb',\n",
       "   ('+/gb',\n",
       "    'brexit/nlp.brexit.noun',\n",
       "    'recriminations/nlp.recrimination.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'n./nlp.n..noun', 'korea/nlp.korea.noun'),\n",
       "  ('+/gb',\n",
       "   'new/nlp.new.adj',\n",
       "   ('+/gb',\n",
       "    'bus/nlp.bus.noun',\n",
       "    ('+/gb',\n",
       "     'to/nlp.to.adp',\n",
       "     'route/nlp.route.noun',\n",
       "     ('+/gb',\n",
       "      'luxury/nlp.luxury.noun',\n",
       "      ('+/gb', 'ski/nlp.ski.noun', 'resort/nlp.resort.noun')))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   \"'s/nlp.'s.part\",\n",
       "   ('+/gb',\n",
       "    'longest/nlp.long.adj',\n",
       "    ('+/gb', 'rail/nlp.rail.noun', 'tunnel/nlp.tunnel.noun')),\n",
       "   'world/nlp.world.noun'),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'switzerland/nlp.switzerland.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'film/nlp.film.noun', 'festival/nlp.festival.noun'),\n",
       "  ('+/gb',\n",
       "   'in/nlp.in.adp',\n",
       "   ('+/gb',\n",
       "    ('+/gb',\n",
       "     'cinema/nlp.cinema.noun',\n",
       "     'saudi/nlp.saudi.noun',\n",
       "     'less/nlp.less.adj'),\n",
       "    'arabia/nlp.arabia.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'wanda/nlp.wanda.noun',\n",
       "   ('+/gb',\n",
       "    'city/nlp.city.noun',\n",
       "    ('+/gb', 'theme/nlp.theme.noun', 'park/nlp.park.noun'))),\n",
       "  ('+/gb',\n",
       "   'in/nlp.in.adp',\n",
       "   ('+/gb',\n",
       "    'a/nlp.a.det',\n",
       "    ('+/gb',\n",
       "     'with/nlp.with.adp',\n",
       "     'battle/nlp.battle.noun',\n",
       "     'disney/nlp.disney.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   \"‘/nlp.'.punct\",\n",
       "   ('+/gb',\n",
       "    'world/nlp.world.noun',\n",
       "    ('+/gb',\n",
       "     '’s/nlp.’s.adj',\n",
       "     ('+/gb',\n",
       "      'largest’/nlp.largest’.adj',\n",
       "      ('+/gb',\n",
       "       'for/nlp.for.adp',\n",
       "       'facility/nlp.facility.noun',\n",
       "       ('+/gb', 'disabled/nlp.disabled.adj', 'kids/nlp.kid.noun')))))),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'jerusalem/nlp.jerusalem.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'queen/nlp.queen.noun',\n",
       "  ('+/gb',\n",
       "   'commonwealth/nlp.commonwealth.noun',\n",
       "   ('+/gb',\n",
       "    'on/nlp.on.adp',\n",
       "    'meeting/nlp.meeting.noun',\n",
       "    'malta/nlp.malta.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'banksy/nlp.banksy.noun',\n",
       "  ('+/gb',\n",
       "   'hotel/nlp.hotel.noun',\n",
       "   ('walled_off/nlp.wall.verb+nlp.off.part',\n",
       "    ('+/gb',\n",
       "     'in/nlp.in.adp',\n",
       "     ('+/gb', 'west/nlp.west.propn', 'bank/nlp.bank.noun')),\n",
       "    ('offering/nlp.offer.verb',\n",
       "     ('+/gb',\n",
       "      'worst/nlp.bad.adj',\n",
       "      ('+/gb',\n",
       "       'in/nlp.in.adp',\n",
       "       'view/nlp.view.noun',\n",
       "       ('+/gb', 'the/nlp.the.det', 'world/nlp.world.noun'))))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'musharraf/nlp.musharraf.noun',\n",
       "  ('+/gb',\n",
       "   'his/nlp.his.adj',\n",
       "   ('+/gb', 'election/nlp.election.noun', 'campaign/nlp.campaign.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'berlin/nlp.berlin.noun',\n",
       "  ('+/gb',\n",
       "   \"'s/nlp.'s.part\",\n",
       "   ('+/gb',\n",
       "    'first/nlp.first.adj',\n",
       "    ('+/gb', 'major/nlp.major.adj', 'centre/nlp.centre.noun')),\n",
       "   'germany/nlp.germany.noun',\n",
       "   ('+/gb', 'gay/nlp.gay.adj', 'refugee/nlp.refugee.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'north/nlp.north.noun', 'korea/nlp.korea.noun'),\n",
       "  ('+/gb',\n",
       "   'international/nlp.international.adj',\n",
       "   'camp/nlp.camp.noun',\n",
       "   ('+/gb', 'for/nlp.for.adp', 'kids/nlp.kid.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'charlie/nlp.charlie.noun',\n",
       "   ('+/gb',\n",
       "    'hebdo/nlp.hebdo.noun',\n",
       "    ('+/gb', 'printing/nlp.printing.noun', 'plant/nlp.plant.noun'))),\n",
       "  ('+/gb',\n",
       "   'for/nlp.for.adp',\n",
       "   ('+/gb',\n",
       "    'first/nlp.first.adj',\n",
       "    ('+/gb',\n",
       "     'since/nlp.since.adp',\n",
       "     'time/nlp.time.noun',\n",
       "     'attacks/nlp.attack.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'german/nlp.german.adj',\n",
       "   ('+/gb', 'federal/nlp.federal.adj', 'prosecutor/nlp.prosecutor.noun')),\n",
       "  ('+/gb',\n",
       "   'official/nlp.official.adj',\n",
       "   ('+/gb',\n",
       "    'into/nlp.into.adp',\n",
       "    'investigation/nlp.investigation.noun',\n",
       "    ('+/gb',\n",
       "     'alleged/nlp.alleged.adj',\n",
       "     ('+/gb',\n",
       "      'nsa/nlp.nsa.noun',\n",
       "      ('+/gb',\n",
       "       'of/nlp.of.adp',\n",
       "       'surveillance/nlp.surveillance.noun',\n",
       "       ('+/gb',\n",
       "        'chancellor/nlp.chancellor.noun',\n",
       "        'phone/nlp.phone.noun',\n",
       "        ('+/gb',\n",
       "         'angela/nlp.angela.propn',\n",
       "         ('+/gb', \"'s/nlp.'s.part\", 'merkel/nlp.merkel.propn'))))))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'paris/nlp.paris.adj', 'prosecutor/nlp.prosecutor.noun'),\n",
       "  ('+/gb',\n",
       "   'into/nlp.into.adp',\n",
       "   'investigation/nlp.investigation.noun',\n",
       "   ('+/gb',\n",
       "    ('+/gb',\n",
       "     'dcns/nlp.dcns.noun',\n",
       "     ('+/gb',\n",
       "      ('+/gb', 'naval/nlp.naval.adj', 'contractor/nlp.contractor.noun'),\n",
       "      'data/nlp.datum.noun')),\n",
       "    'leak/nlp.leak.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'largest/nlp.large.adj',\n",
       "   ('+/gb',\n",
       "    'in/nlp.in.adp',\n",
       "    'building/nlp.building.noun',\n",
       "    ('+/gb', 'the/nlp.the.det', 'world/nlp.world.noun'))),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'china/nlp.china.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'albania/nlp.albania.noun',\n",
       "  ('+/gb',\n",
       "   'huge/nlp.huge.adj',\n",
       "   'bunker/nlp.bunker.noun',\n",
       "   ('+/gb', 'cold/nlp.cold.adj', 'war/nlp.war.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'liberal/nlp.liberal.adj',\n",
       "   ('+/gb',\n",
       "    'mosque/nlp.mosque.noun',\n",
       "    ('+/gb', 'taboo/nlp.taboo.noun', 'breaking/nlp.break.verb'))),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'berlin/nlp.berlin.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'israel/nlp.israel.noun',\n",
       "  ('+/gb',\n",
       "   'first/nlp.first.adj',\n",
       "   ('+/gb',\n",
       "    'of/nlp.of.adp',\n",
       "    'investigation/nlp.investigation.noun',\n",
       "    ('+/gb',\n",
       "     'senior/nlp.senior.adj',\n",
       "     ('+/gb',\n",
       "      'over/nlp.over.adp',\n",
       "      'officer/nlp.officer.noun',\n",
       "      ('+/gb', 'gaza/nlp.gaza.propn', 'war/nlp.war.noun')))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   'mexican/nlp.mexican.adj',\n",
       "   ('+/gb', 'high/nlp.high.adj', 'court/nlp.court.noun')),\n",
       "  ('+/gb',\n",
       "   ('+/gb', 'to/nlp.to.adp', 'door/nlp.door.noun'),\n",
       "   ('legalizing/nlp.legalize.verb',\n",
       "    ('+/gb', 'marijuana/nlp.marijuana.noun', 'cnn.com/nlp.cnn.com.x')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'vladimir/nlp.vladimir.noun', 'putin/nlp.putin.noun'),\n",
       "  ('+/gb',\n",
       "   'russian/nlp.russian.adj',\n",
       "   ('+/gb', 'patriot/nlp.patriot.noun', 'park/nlp.park.noun'),\n",
       "   ('+/gb', 'military/nlp.military.adj', 'disneyland/nlp.disneyland.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'india/nlp.india.noun',\n",
       "  ('+/gb',\n",
       "   'first/nlp.first.adj',\n",
       "   ('+/gb',\n",
       "    'for/nlp.for.adp',\n",
       "    'school/nlp.school.noun',\n",
       "    ('+/gb', 'transgender/nlp.transgender.noun', 'pupils/nlp.pupil.noun')),\n",
       "   ('+/gb', 'bbc/nlp.bbc.noun', 'news/nlp.news.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb', 'north/nlp.north.noun', 'korea/nlp.korea.noun'),\n",
       "  ('+/gb',\n",
       "   'summer/nlp.summer.noun',\n",
       "   'camp/nlp.camp.noun',\n",
       "   ('+/gb', 'for/nlp.for.adp', 'kids/nlp.kid.noun'))),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   \"'s/nlp.'s.part\",\n",
       "   ('+/gb',\n",
       "    'longest/nlp.long.adj',\n",
       "    ('+/gb',\n",
       "     'pedestrian/nlp.pedestrian.noun',\n",
       "     ('+/gb', 'suspension/nlp.suspension.noun', 'bridge/nlp.bridge.noun'))),\n",
       "   'world/nlp.world.noun'),\n",
       "  ('+/gb', 'in/nlp.in.adp', 'switzerland/nlp.switzerland.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  ('+/gb',\n",
       "   \"'s/nlp.'s.part\",\n",
       "   'parliament/nlp.parliament.noun',\n",
       "   'burma/nlp.burma.noun'),\n",
       "  ('+/gb',\n",
       "   'in/nlp.in.adp',\n",
       "   ('+/gb',\n",
       "    'the/nlp.the.det',\n",
       "    ('+/gb',\n",
       "     'of/nlp.of.adp',\n",
       "     'dawning/nlp.dawning.noun',\n",
       "     ('+/gb',\n",
       "      'a/nlp.a.det',\n",
       "      ('+/gb',\n",
       "       'new/nlp.new.adj',\n",
       "       ('+/gb', 'democratic/nlp.democratic.adj', 'era/nlp.era.noun'))))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'france/nlp.france.noun',\n",
       "  ('+/gb',\n",
       "   'bribery/nlp.bribery.noun',\n",
       "   ('+/gb',\n",
       "    'into/nlp.into.adp',\n",
       "    'investigation/nlp.investigation.noun',\n",
       "    ('+/gb', 'gabonese/nlp.gabonese.adj', 'official/nlp.official.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'china/nlp.china.noun',\n",
       "  ('+/gb',\n",
       "   'solar/nlp.solar.adj',\n",
       "   ('+/gb',\n",
       "    'plant/nlp.plant.noun',\n",
       "    ('+/gb',\n",
       "     'power/nlp.power.noun',\n",
       "     ('+/gb', 'panda/nlp.panda.noun', 'shaped/nlp.shape.verb'))))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'paris/nlp.paris.noun',\n",
       "  ('+/gb',\n",
       "   'art/nlp.art.noun',\n",
       "   ('glorifying/nlp.glorify.verb',\n",
       "    'exhibit/nlp.exhibit.noun',\n",
       "    ('+/gb', 'suicide/nlp.suicide.noun', 'bombers/nlp.bomber.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'eu/nlp.eu.x',\n",
       "  ('+/gb',\n",
       "   'over/nlp.over.adp',\n",
       "   ('+/gb',\n",
       "    'legal/nlp.legal.adj',\n",
       "    ('+/gb',\n",
       "     'against/nlp.against.adp',\n",
       "     'case/nlp.case.noun',\n",
       "     ('+/gb', 'warsaw/nlp.warsaw.noun', 'budapest/nlp.budapest.noun'))),\n",
       "   'prague/nlp.prague.noun',\n",
       "   'migration/nlp.migration.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'panama/nlp.panama.noun',\n",
       "  ('+/gb', 'biodiversity/nlp.biodiversity.noun', 'museum/nlp.museum.noun')),\n",
       " ('opens/nlp.open.verb',\n",
       "  'eu/nlp.eu.noun',\n",
       "  ('+/gb',\n",
       "   'antitrust/nlp.antitrust.adj',\n",
       "   ('+/gb',\n",
       "    'against/nlp.against.adp',\n",
       "    'case/nlp.case.noun',\n",
       "    ('+/gb',\n",
       "     \"'s/nlp.'s.part\",\n",
       "     ('+/gb', 'gas/nlp.ga.noun', 'giant/nlp.giant.noun'),\n",
       "     'russia/nlp.russia.noun',\n",
       "     'gazprom/nlp.gazprom.noun')))),\n",
       " ('opens/nlp.open.verb',\n",
       "  'china/nlp.china.noun',\n",
       "  ('+/gb',\n",
       "   'the/nlp.the.det',\n",
       "   ('+/gb',\n",
       "    'biggest/nlp.big.adj',\n",
       "    ('+/gb',\n",
       "     'in/nlp.in.adp',\n",
       "     'building/nlp.build.verb',\n",
       "     ('+/gb', 'the/nlp.the.det', 'world/nlp.world.noun'))),\n",
       "   'stand_alone/nlp.stand.verb+nlp.alone.adv'))}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = hg.pattern2edges(['opens/nlp.open.verb', None, None])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sayers, sayers + claims & sorted sayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sayers = {}\n",
    "sayers_and_claims = {}\n",
    "for edge in say_edges:\n",
    "    sayer = edge2syn(edge[1])\n",
    "    if sayer not in sayers_and_claims:\n",
    "        sayers[sayer] = 0\n",
    "        sayers_and_claims[sayer] = []\n",
    "    sayers[sayer] += 1\n",
    "    sayers_and_claims[sayer].append(edge[2])\n",
    "        \n",
    "sorted_sayers = sorted(sayers.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepts by sayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concepts_by_sayer = {}\n",
    "\n",
    "\n",
    "def add_concepts(targ, src):\n",
    "    for key in src:\n",
    "        if key in targ:\n",
    "            targ[key] += src[key]\n",
    "        else:\n",
    "            targ[key] = src[key]\n",
    "\n",
    "\n",
    "def concepts_in_claim(claim, concept_map=None, deep=True):\n",
    "    if not concept_map:\n",
    "        concept_map = {}\n",
    "    syn_id = edge2syn(claim)\n",
    "    if syn_id:\n",
    "        if syn_id not in concept_map:\n",
    "            concept_map[syn_id] = 0\n",
    "        concept_map[syn_id] += 1\n",
    "        \n",
    "        if deep:\n",
    "            if sym.is_edge(claim):\n",
    "                for item in claim[1:]:\n",
    "                    concepts_in_claim(item, concept_map)\n",
    "    return concept_map\n",
    "\n",
    "\n",
    "def get_concepts_by_sayer(sayer, that_include=None):\n",
    "    concept_map = {}\n",
    "    for claim in sayers_and_claims[sayer]:\n",
    "        claim_concepts = concepts_in_claim(claim)\n",
    "        if not that_include:\n",
    "            add_concepts(concept_map, claim_concepts)\n",
    "        elif that_include in claim_concepts.keys():\n",
    "            del claim_concepts[that_include]\n",
    "            add_concepts(concept_map, claim_concepts)\n",
    "    return concept_map\n",
    "\n",
    "\n",
    "for sayer in sayers_and_claims:\n",
    "    concepts_by_sayer[sayer] = get_concepts_by_sayer(sayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who talks about who graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "who_who = {}\n",
    "\n",
    "actors = [sayer for sayer in sayers if sayer and len(sayers_and_claims[sayer]) > 1]\n",
    "active_actors = set()\n",
    "\n",
    "def add_edge(orig, targ):\n",
    "    if orig and targ:\n",
    "        if orig not in who_who:\n",
    "            who_who[orig] = {}\n",
    "        if targ not in who_who[orig]:\n",
    "            who_who[orig][targ] = 0\n",
    "        who_who[orig][targ] += 1\n",
    "        if orig != targ:\n",
    "            active_actors.add(orig)\n",
    "            active_actors.add(targ)\n",
    "\n",
    "for sayer in actors:\n",
    "    for claim in sayers_and_claims[sayer]:\n",
    "        claim_concepts = concepts_in_claim(claim)\n",
    "        for concept in claim_concepts:\n",
    "            if concept in actors:\n",
    "                add_edge(sayer, concept)\n",
    "            \n",
    "file = open('../who_who.gml', 'w')\n",
    "file.write('graph\\n[\\n')\n",
    "for actor in active_actors:\n",
    "    # print(actor)\n",
    "    file.write('node\\n[\\nid %s\\nlabel \"%s\"\\n]\\n' % (str(actor), mer.synonym_label(actor, short=True)))\n",
    "for orig in who_who:\n",
    "    for targ in who_who[orig]:\n",
    "        w = who_who[orig][targ]\n",
    "        file.write('edge\\n[\\nsource %s\\ntarget %s\\nweight %s\\n]\\n' % (str(orig), str(targ), str(w)))\n",
    "file.write(']\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept-actor graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concept_actor = {}\n",
    "\n",
    "def add_edge(orig, targ):\n",
    "    if orig and targ:\n",
    "        if orig not in concept_actor:\n",
    "            concept_actor[orig] = {}\n",
    "        if targ not in concept_actor[orig]:\n",
    "            concept_actor[orig][targ] = 0\n",
    "        concept_actor[orig][targ] += 1\n",
    "\n",
    "def actors_for_concept(concept):\n",
    "    return concept_actor[concept].keys()\n",
    "        \n",
    "for sayer in actors:\n",
    "    for claim in sayers_and_claims[sayer]:\n",
    "        claim_concepts = concepts_in_claim(claim)\n",
    "        for concept in claim_concepts:\n",
    "            add_edge(concept, sayer)\n",
    "\n",
    "concept_metrics = {}\n",
    "            \n",
    "for concept in concept_actor:\n",
    "    weights = [concept_actor[concept][actor] for actor in concept_actor[concept]]\n",
    "    total = sum(weights)\n",
    "    h_weights = [float(i) / float(total) for i in weights]\n",
    "    h_weights = [i * i for i in h_weights]\n",
    "    h = 1. / sum(h_weights)\n",
    "    concept_metrics[concept] = {'total': total, 'h': h}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispersion of Concepts amongst Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x3bcf27978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [concept_metrics[concept]['h'] for concept in concept_metrics]\n",
    "x = [i for i in x if i < 25]\n",
    "\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(x, 50, log=True, facecolor='green', alpha=0.75)\n",
    "\n",
    "plt.xlabel('1 / herfindhal')\n",
    "plt.ylabel('Frequency (log)')\n",
    "plt.title('Dispersion of Concepts amongst Actors')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Top Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2639205 {russia} 266 1161\n",
      "134 {korea, (+ north korea)} 135 507\n",
      "2639483 {putin} 128 541\n",
      "2638657 {iran} 128 460\n",
      "2638503 {turkey} 100 472\n",
      "322 {(+ pope francis), francis, pope} 79 351\n",
      "2638654 {ukraine} 79 298\n",
      "2639270 {france} 59 262\n",
      "2641341 {erdogan} 57 236\n",
      "920 {(+ david cameron), cameron, david} 56 226\n",
      "2652235 {pentagon} 56 260\n",
      "472 {(+ saudi arabia), saudi, arabia} 55 259\n",
      "2638372 {syria} 53 199\n",
      "2643138 {kerry} 52 214\n",
      "507 {kremlin, (+ the kremlin)} 49 245\n",
      "2638559 {pakistan} 48 197\n",
      "54 {angela, merkel, (+ angela merkel)} 46 215\n",
      "2638454 {germany} 44 246\n",
      "2638467 {eu} 40 181\n",
      "2638636 {japan} 40 175\n"
     ]
    }
   ],
   "source": [
    "for t in sorted_sayers[:21]:\n",
    "    syn_id = t[0]\n",
    "    if syn_id:\n",
    "        print('%s %s %s %s' % (syn_id, mer.synonym_label(syn_id), t[1], len(concepts_by_sayer[syn_id])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepts by actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2638715] {missile} 19 9.102362204724404\n",
      "[20] {(+ the u.s), u.s} 10 28.508474576271112\n",
      "[134] {korea, (+ north korea)} 7 31.970168612191955\n",
      "[989] {(+ ballistic missile), ballistic} 7 3.755555555555556\n",
      "[2644190] {kim} 6 10.88059701492538\n",
      "[1598] {(+ jong un), jong} 6 9.965517241379311\n",
      "[107093] {(+ hostile acts’), (+ it has_arrested (+ a (+ virginia (+ for student (+ hostile acts’))))), (+ a (+ virginia (+ for student (+ hostile acts’)))), (+ for student (+ hostile acts’)), (+ broad (+ ‘ (+ terrorist (+ acts’ list)))), (+ terrorist (+ acts’ list)), acts’, (+ acts’ list), (+ ‘ (+ terrorist (+ acts’ list))), (+ virginia (+ for student (+ hostile acts’)))} 6 1.0\n",
      "[1987264] {(+ joint (+ industrial zone)), (+ industrial zone)} 5 1.0\n",
      "[2639728] {(+ nuclear test)} 5 2.1333333333333333\n",
      "[4554] {declaration, (+ of declaration war), (+ a (+ of declaration war))} 5 2.7777777777777772\n",
      "[2326389] {(+ no (+ to ties (+ rogue (+ ship nytimes.com)))), (+ to ties (+ rogue (+ ship nytimes.com))), (+ ship nytimes.com), (+ rogue (+ ship nytimes.com)), (has it (+ no (+ to ties (+ rogue (+ ship nytimes.com)))))} 5 1.0\n",
      "[1965597] {(+ the us), (+ in (+ the us))} 4 8.0\n",
      "[2073012] {(+ at test (+ any (+ time (+ news agency official)))), (+ news agency official), (+ any (+ time (+ news agency official))), (+ time (+ news agency official))} 4 1.0\n",
      "[2653450] {(+ south korea)} 4 6.999999999999999\n",
      "[2651848] {sony} 4 2.9999999999999996\n",
      "[1995864] {(+ worst drought), (+ by (+ its (+ worst drought))), (+ its (+ worst drought))} 4 1.0\n",
      "[2210902] {(+ against act country), (+ hostile (+ against act country)), (+ for (committing (+ hostile (+ against act country)))), (committing (+ hostile (+ against act country)))} 4 1.0\n",
      "[3297] {(+ with (+ multiple (+ nuclear warheads))), (+ nuclear warheads), warheads, (+ multiple (+ nuclear warheads))} 4 1.0\n",
      "[2646221] {submarine} 4 2.909090909090909\n",
      "[2062705] {(+ prepared (to_respond_to (+ nuclear attack (+ by us)) (+ in kind))), (+ nuclear attack (+ by us)), (to_respond_to (+ nuclear attack (+ by us)) (+ in kind)), (+ in kind)} 4 1.0\n",
      "[2124414] {(to_take (+ 's (to advice start talks) china)), (to advice start talks), (+ 's (to advice start talks) china), (+ willing (to_take (+ 's (to advice start talks) china)))} 4 1.0\n"
     ]
    }
   ],
   "source": [
    "concepts = concepts_by_sayer[134]\n",
    "\n",
    "sorted_concepts = sorted(concepts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "for t in sorted_concepts[:21]:\n",
    "    syn_id = t[0]\n",
    "    if syn_id:\n",
    "        print('[%s] %s %s %s' % (syn_id, mer.synonym_label(syn_id, short=False), t[1], concept_metrics[syn_id]['h']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepts by actor and $1/herfindhal$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[987] new york 4.0 4 {ukraine, iraq pm, belgium}\n",
      "[1220458] hackers 4.5 6 {a new report, cybersecurity firm, vodafone, yahoo}\n",
      "[1220267] spies 4.0 4 {putin, norway, sweden}\n",
      "[1220437] poland 3.571428571428571 5 {britain, putin, defense minister}\n",
      "[1222383] foreign minister 3.571428571428571 5 {iranian mp, iraq, iranian fm}\n",
      "[5357] armenian genocide 4.0 4 {turkey, watch human rights, erdoğan}\n",
      "[1230107] jordan 3.571428571428571 5 {syrian government, japan, the syrian army}\n",
      "[1222061] in ukraine 4.499999999999999 6 {angela merkel, kiev, osce, george soros}\n",
      "[16] the cia 4.5 6 {european court, wikileaks, german government, pakistan court}\n",
      "[1271329] patience 3.571428571428571 5 {turkey, germany, trump}\n",
      "[1243094] with u.s 4.499999999999999 6 {iran, khamenei, germany, 's lavrov russia}\n",
      "[1223876] diplomatic 4.0 4 {the kremlin, turkey, russian official}\n",
      "[1227262] commander 4.0 4 {hezbollah, iraqi military, iraq}\n",
      "[1996] the brink 3.571428571428571 5 {north korea, france, putin}\n",
      "[2056] zika virus 4.0 4 {health ministry, cdc, mexico}\n",
      "[76] the mediterranean 4.0 4 {italian pm, italy, sinking of mediterranean mass murder}\n",
      "[1225681] russian forces 3.571428571428571 5 {ukraine, 's president ukraine, nato commander}\n",
      "[1225952] airlines 4.0 4 {ukraine, tony abbott, pm tony abbott}\n",
      "[1238590] tribunal 4.0 4 {foreign minister, putin, julie bishop}\n",
      "[3315] pro separatists russian 4.499999999999999 6 {ukraine, kerry, putin, rebel leader}\n"
     ]
    }
   ],
   "source": [
    "ego = 1221043\n",
    "concepts = concepts_by_sayer[ego]\n",
    "\n",
    "concepts_by_h = {}\n",
    "for concept in concepts:\n",
    "    h = int(round(concept_metrics[concept]['h']))\n",
    "    if h not in concepts_by_h:\n",
    "        concepts_by_h[h] = []\n",
    "    concepts_by_h[h].append(concept)\n",
    "\n",
    "for syn_id in concepts_by_h[4][:20]:\n",
    "    actors = [mer.synonym_label(actor, short=True) for actor in actors_for_concept(syn_id) if actor != ego]\n",
    "    actor_str = ', '.join(actors)\n",
    "    print('[%s] %s %s %s {%s}' % (syn_id, mer.synonym_label(syn_id, short=True), concept_metrics[syn_id]['h'], concept_metrics[syn_id]['total'], actor_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment on concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ego = 845         # the pope\n",
    "concept = 959230  # contraception\n",
    "\n",
    "def related_concepts(actor, concept):\n",
    "    result = set()\n",
    "    for claim in sayers_and_claims[actor]:\n",
    "        claim_concepts = concepts_in_claim(claim)\n",
    "        if concept in claim_concepts:\n",
    "            for c in claim_concepts:\n",
    "                if c != concept:\n",
    "                    result.add(c)\n",
    "    return result\n",
    "\n",
    "actors = actors_for_concept(concept)\n",
    "\n",
    "for actor in actors:\n",
    "    actor_name = mer.synonym_label(actor, short=True)\n",
    "    print('ACTOR: %s' % actor_name)\n",
    "    related = related_concepts(actor, concept)\n",
    "    for rel in related:\n",
    "        print(mer.synonym_label(rel, short=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who talks about what - Ego Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ego = 845\n",
    "who_what = {}\n",
    "\n",
    "concepts = concepts_by_sayer[ego]\n",
    "\n",
    "nodes = set()\n",
    "\n",
    "def add_edge(orig, targ, weight):\n",
    "    if orig and targ:\n",
    "        if orig not in who_what:\n",
    "            who_what[orig] = {}\n",
    "        who_what[orig][targ] = weight\n",
    "        nodes.add(orig)\n",
    "        nodes.add(targ)\n",
    "\n",
    "for concept in concepts:\n",
    "    if len(concept_actor[concept]) > 1:\n",
    "        for actor in concept_actor[concept]:\n",
    "            add_edge(actor, concept, concept_actor[concept][actor])\n",
    "        \n",
    "file = open('../who_what.gml', 'w')\n",
    "file.write('graph\\n[\\n')\n",
    "for node in nodes:\n",
    "    file.write('node\\n[\\nid %s\\nlabel \"%s\"\\n]\\n' % (str(node), mer.synonym_label(node, short=True)))\n",
    "for orig in who_what:\n",
    "    for targ in who_what[orig]:\n",
    "        w = who_what[orig][targ]\n",
    "        file.write('edge\\n[\\nsource %s\\ntarget %s\\nweight %s\\n]\\n' % (str(orig), str(targ), str(w)))\n",
    "file.write(']\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ego = 900574\n",
    "\n",
    "concepts = concepts_by_sayer[ego]\n",
    "\n",
    "file = open('../france.csv', 'w')\n",
    "file.write('concept,herfindahl,total,ego_total,actors\\n')\n",
    "for concept in concepts:\n",
    "    file.write('%s,%s,%s,%s,' % (mer.synonym_label(concept, short=True),\n",
    "                                 concept_metrics[concept]['h'],\n",
    "                                 concept_metrics[concept]['total'],\n",
    "                                 concept_actor[concept][ego]))\n",
    "    actor_str = '|'.join([mer.synonym_label(actor, short=True) for actor in concept_actor[concept] if actor != ego])\n",
    "    file.write('%s\\n' % actor_str)\n",
    "file.write('\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(mer.synonym_label(908171))\n",
    "print(mer.syn_id('(+ pope francis)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept proximity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0056965782477258"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs = HyperSimilarity(hg)\n",
    "\n",
    "global_economy = mer.syn_id('(+ global economy)')\n",
    "financial_crisis = mer.syn_id('(+ financial crisis)')\n",
    "catholic_church = mer.syn_id('(+ catholic church)')\n",
    "pope_francis = mer.syn_id('(+ pope francis)')\n",
    "hackers = mer.syn_id('hackers')\n",
    "spies = mer.syn_id('spies')\n",
    "\n",
    "# mer.synonym_full_edges(c4)\n",
    "\n",
    "hs.synonym_similarity(mer, hackers, spies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "used_concepts = set()\n",
    "\n",
    "for sayer in sayers_and_claims:\n",
    "    used_concepts = used_concepts.union(concepts_by_sayer[sayer])\n",
    "\n",
    "used_concepts = list(used_concepts)\n",
    "    \n",
    "print(len(used_concepts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed()\n",
    "\n",
    "def random_concept():\n",
    "    return random.choice(used_concepts)\n",
    "\n",
    "x = []\n",
    "best_prox = -1.\n",
    "best_pair = None\n",
    "for i in range(1000):\n",
    "    c1 = random_concept()\n",
    "    c2 = random_concept()\n",
    "    hs = HyperSimilarity(hg)\n",
    "    prox = hs.synonym_similarity(mer, c1, c2)\n",
    "    x.append(prox)\n",
    "    if prox > best_prox:\n",
    "        best_prox = prox\n",
    "        best_pair = (c1, c2)\n",
    "        print('%s | %s => %s' % (mer.synonym_label(best_pair[0], short=True), mer.synonym_label(best_pair[1], short=True), best_prox))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTOR new report\n",
      "-> russian hackers\n",
      "-> of version microsoft windows\n",
      "-> used_to russian hackers flaw in microsoft windows spy on nato\n",
      "-> russian\n",
      "-> on nato\n",
      "-> nato\n",
      "-> flaw in microsoft windows\n",
      "-> in microsoft windows\n",
      "ACTOR putin\n",
      "-> russian\n",
      "-> russian hackers\n",
      "-> private russian hackers\n",
      "ACTOR 's defence ministry china\n",
      "-> us hackers\n",
      "ACTOR noaa\n",
      "-> of breached four by chinese hackers its websites\n",
      "-> breached four by chinese hackers\n",
      "-> chinese hackers\n",
      "-> chinese\n",
      "-> its websites\n",
      "ACTOR at hunter 's largest cybersecurity firm russia\n",
      "-> chinese hackers\n",
      "-> chinese\n",
      "-> us chinese hackers\n",
      "ACTOR in article the wall street journal\n",
      "-> cyber attack\n",
      "-> citizen ’s arrest\n",
      "-> has_label – us –_us\n",
      "-> of focus recent in surge cyber attacks that could_be_linked to iranian ’s arrest american\n",
      "-> chinese\n",
      "-> recent in surge cyber attacks\n",
      "-> that could_be_linked to iranian ’s arrest american\n",
      "-> to iranian ’s arrest american\n",
      "-> appear – us target obama administration accounts iran military hackers working officials on iran policy to_be of focus recent in surge cyber attacks that could_be_linked to iranian ’s arrest american\n",
      "-> working officials on iran policy\n",
      "-> on iran policy\n",
      "-> target obama administration accounts\n",
      "-> to_be of focus recent in surge cyber attacks that could_be_linked to iranian ’s arrest american\n",
      "-> 's iran policy netanyahu\n",
      "-> in surge cyber attacks\n",
      "-> cyber attacks\n",
      "-> it too_was_hit by chinese hackers\n",
      "-> too_was_hit\n",
      "-> could_be_linked\n",
      "-> iran\n",
      "-> military hackers\n",
      "-> iranian\n",
      "-> by chinese hackers\n",
      "-> iranian ’s arrest american\n",
      "-> chinese hackers\n",
      "ACTOR vodafone\n",
      "-> into customer accounts nearly 2,000\n",
      "-> nearly 2,000\n",
      "-> broke hackers into customer accounts nearly 2,000 this week\n",
      "-> this week\n",
      "-> accounts nearly 2,000\n",
      "-> customer accounts nearly 2,000\n",
      "-> than 2,000\n",
      "ACTOR yahoo\n",
      "-> from users about million 500 in 2014\n",
      "-> users about million 500 in 2014\n",
      "-> about million 500\n",
      "-> cyber attack\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6178b49d221f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ACTOR %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynonym_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mconcept\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrelated_concepts_by_actor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-> %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynonym_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/projects/graphbrain/gb/synonyms/meronomy.py\u001b[0m in \u001b[0;36msynonym_label\u001b[0;34m(self, syn_id, short)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     \u001b[0mbest_edge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                     \u001b[0mbest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0med\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0medge2label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_edge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\"'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m'{%s}'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matom\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0matom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynonym_sets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msyn_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/graphbrain/gb/synonyms/meronomy.py\u001b[0m in \u001b[0;36medge2label\u001b[0;34m(edge)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_edge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0m_edge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_edge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medge2label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_edge\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/graphbrain/gb/synonyms/meronomy.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_edge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0m_edge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_edge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medge2label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_edge\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/graphbrain/gb/synonyms/meronomy.py\u001b[0m in \u001b[0;36medge2label\u001b[0;34m(edge)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_edge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0m_edge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_edge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medge2label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_edge\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/graphbrain/gb/synonyms/meronomy.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_edge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0m_edge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_edge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medge2label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_edge\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/graphbrain/gb/synonyms/meronomy.py\u001b[0m in \u001b[0;36medge2label\u001b[0;34m(edge)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_edge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0m_edge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_edge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medge2label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_edge\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/graphbrain/gb/synonyms/meronomy.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_edge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0m_edge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_edge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medge2label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_edge\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/graphbrain/gb/synonyms/meronomy.py\u001b[0m in \u001b[0;36medge2label\u001b[0;34m(edge)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0m_edge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_edge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_edge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0m_edge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0m_edge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_edge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medge2label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_edge\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "def related_concepts(main_concept, actor):\n",
    "    concepts = set()\n",
    "    for claim in sayers_and_claims[actor]:\n",
    "        claim_concepts = concepts_in_claim(claim)\n",
    "        if main_concept in claim_concepts:\n",
    "            for concept in claim_concepts:\n",
    "                if concept != main_concept:\n",
    "                    concepts.add(concept)\n",
    "    return concepts\n",
    "\n",
    "# main_concept = mer.syn_id('cia')\n",
    "# main_concept = mer.syn_id('fifa')\n",
    "main_concept = mer.syn_id('hackers')\n",
    "actors = actors_for_concept(main_concept)\n",
    "\n",
    "related_concepts_by_actor = {}\n",
    "for actor in actors:\n",
    "    related_concepts_by_actor[actor] = related_concepts(main_concept, actor)\n",
    "    \n",
    "for actor in related_concepts_by_actor:\n",
    "    print('ACTOR %s' % mer.synonym_label(actor, short=True))\n",
    "    for concept in related_concepts_by_actor[actor]:\n",
    "        print('-> %s' % mer.synonym_label(concept, short=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new report -> russian [3759.6564149388964]\n",
      "putin -> russian [3759.6564149388964]\n",
      "'s defence ministry china -> iran [0.44451117516218647]\n",
      "noaa -> chinese [2025.0299948391785]\n",
      "at hunter 's largest cybersecurity firm russia -> chinese [2025.0299948391785]\n",
      "in article the wall street journal -> chinese [2025.0299948391785]\n",
      "vodafone -> iran [74.04284835738136]\n",
      "yahoo -> cyber attack [308.26321139407975]\n"
     ]
    }
   ],
   "source": [
    "hs = HyperSimilarity(hg)\n",
    "\n",
    "def closest_concept(orig_set, targ_set):\n",
    "    best_prox = -1.\n",
    "    best_pair = None\n",
    "    for orig in orig_set:\n",
    "        for targ in targ_set:\n",
    "            prox = hs.synonym_similarity(mer, orig, targ)\n",
    "            if prox > best_prox:\n",
    "                best_prox = prox\n",
    "                best_pair = (orig, targ)\n",
    "    return best_pair, best_prox\n",
    "\n",
    "nodes = {}\n",
    "edges = []\n",
    "for actor in related_concepts_by_actor:\n",
    "    nodes[actor] = {'type': 'actor'}\n",
    "for actor_orig in related_concepts_by_actor:\n",
    "    best_prox = -1.\n",
    "    best_pair = None\n",
    "    for actor_targ in related_concepts_by_actor:\n",
    "        if actor_orig != actor_targ:\n",
    "            pair, prox = closest_concept(related_concepts_by_actor[actor_orig], related_concepts_by_actor[actor_targ])\n",
    "            if prox > best_prox:\n",
    "                best_prox = prox\n",
    "                best_pair = pair\n",
    "            pair, prox = closest_concept(related_concepts_by_actor[actor_orig], {actor_targ})\n",
    "            if prox > best_prox:\n",
    "                best_prox = prox\n",
    "                best_pair = pair\n",
    "    concept = best_pair[1]\n",
    "    if concept not in related_concepts_by_actor:\n",
    "        nodes[concept] = {'type': 'concept'}\n",
    "    edges.append((actor_orig, concept))\n",
    "    print('%s -> %s [%s]' % (mer.synonym_label(actor_orig, short=True), mer.synonym_label(concept, short=True), best_prox))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nodes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-26ccca171ea7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0medge\u001b[0m \u001b[0;32min\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nodes' is not defined"
     ]
    }
   ],
   "source": [
    "G = nx.DiGraph()\n",
    "for node in nodes:\n",
    "    G.add_node(node)\n",
    "for edge in edges:\n",
    "    G.add_edge(edge[0], edge[1])\n",
    "gnodes = G.nodes()\n",
    "gedges = G.edges()\n",
    "\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "def node_shape(n):\n",
    "    if nodes[n]['type'] != 'actor':\n",
    "        return 'square'\n",
    "    else:\n",
    "        return 'triangle'\n",
    "    \n",
    "def node_color(n):\n",
    "    if nodes[n]['type'] != 'actor':\n",
    "        return '#8A2BE2'\n",
    "    else:\n",
    "        return '#FFA500'\n",
    "\n",
    "nodes_dict = [{'id': mer.synonym_label(n, short=True),\n",
    "               'color': node_color(n),\n",
    "               'node_shape': node_shape(n),\n",
    "               'x': pos[n][0] * 1000,\n",
    "               'y': pos[n][1] * 1000} for n in gnodes]\n",
    "node_map = dict(zip(nodes,range(len(gnodes))))  # map to indices for source/target in edges\n",
    "\n",
    "edges_dict = [{'source': node_map[gedges[i][0]],\n",
    "               'target': node_map[gedges[i][1]],\n",
    "               'color':'#A0A0A0'} for i in range(len(gedges))]\n",
    "\n",
    "visJS_module.visjs_network(nodes_dict, edges_dict,\n",
    "                           node_size_multiplier=7,\n",
    "                           node_size_transform = '',\n",
    "                           node_font_color = '#303030',\n",
    "                           node_font_background = 'rgba(255, 255, 255, .5)',\n",
    "                           node_font_size=25,\n",
    "                           edge_arrow_to=True,\n",
    "                           physics_enabled=True,\n",
    "                           edge_color_highlight='#8A324E',\n",
    "                           edge_color_hover='#8BADD3',\n",
    "                           edge_width=3,\n",
    "                           max_velocity=15,\n",
    "                           min_velocity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## SemBubbles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actors: 350; concepts: 824; 824\n"
     ]
    }
   ],
   "source": [
    "json = {'actors': {}, 'concepts': {}, 'aa': {}, 'ac': {}, 'ca': {}, 'cc': {}}\n",
    "\n",
    "actors = set([sayer for sayer in sayers if sayer and len(sayers_and_claims[sayer]) > 2])\n",
    "\n",
    "# actors, concepts & aa\n",
    "for actor_id in actors:\n",
    "    actor = mer.synonym_label(actor_id, short=True)\n",
    "    json['actors'][actor] = {'frequency': len(sayers_and_claims[actor_id]),\n",
    "                             'name': actor}\n",
    "    json['aa'][actor] = {}\n",
    "    for concept_id in concepts_by_sayer[actor_id]:\n",
    "        concept = mer.synonym_label(concept_id, short=True)\n",
    "        # concept is actor\n",
    "        if concept_id in actors:\n",
    "            actor2 = concept\n",
    "            if actor2 not in json['aa'][actor]:\n",
    "                json['aa'][actor][actor2] = 0\n",
    "            json['aa'][actor][actor2] += 1\n",
    "        # concept is concept\n",
    "        else:\n",
    "            if concept not in json['concepts']:\n",
    "                json['concepts'][concept] = {'frequency': 0,\n",
    "                                             'name': concept}\n",
    "            json['concepts'][concept]['frequency'] += 1\n",
    "\n",
    "# filter low freq concepts\n",
    "new_concepts = {}\n",
    "for concept in json['concepts']:\n",
    "    if json['concepts'][concept]['frequency'] > 1:\n",
    "        new_concepts[concept] = json['concepts'][concept]\n",
    "json['concepts'] = new_concepts\n",
    "\n",
    "# ac, ca, init cc\n",
    "for actor_id in actors:\n",
    "    actor = mer.synonym_label(actor_id, short=True)\n",
    "    json['ac'][actor] = {}\n",
    "    for concept_id in concepts_by_sayer[actor_id]:\n",
    "        concept = mer.synonym_label(concept_id, short=True)\n",
    "        if concept in json['concepts'] and concept not in actors:\n",
    "            if concept not in json['ca']:\n",
    "                json['ca'][concept] = {}\n",
    "                json['cc'][concept] = {}\n",
    "            if actor not in json['ca'][concept]:\n",
    "                json['ca'][concept][actor] = 0\n",
    "            json['ca'][concept][actor] += 1\n",
    "            if concept not in json['ac'][actor]:\n",
    "                json['ac'][actor][concept] = 0\n",
    "            json['ac'][actor][concept] += 1\n",
    "\n",
    "# cc\n",
    "for actor in actors:\n",
    "    for claim in sayers_and_claims[actor]:\n",
    "        claim_concepts = [mer.synonym_label(concept, short=True) for concept in concepts_in_claim(claim)]\n",
    "        claim_concepts = set([concept for concept in claim_concepts if concept in json['concepts']])\n",
    "        combs = itertools.combinations(claim_concepts, 2)\n",
    "        for comb in combs:\n",
    "            c1 = comb[0]\n",
    "            c2 = comb[1]\n",
    "            if c2 not in json['cc'][c1]:\n",
    "                json['cc'][c1][c2] = 0\n",
    "            if c1 not in json['cc'][c2]:\n",
    "                json['cc'][c2][c1] = 0\n",
    "            json['cc'][c1][c2] += 1\n",
    "            json['cc'][c2][c1] += 1\n",
    "\n",
    "with open('../reddit-worldnews-sembubble.json', 'w') as json_file:\n",
    "    json_file.write(j.dumps(json))\n",
    "\n",
    "print('actors: %s; concepts: %s; %s' % (len(actors), len(json['concepts']), len(json['ca'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actors: 349; concepts: 824; 349\n"
     ]
    }
   ],
   "source": [
    "print('actors: %s; concepts: %s; %s' % (len(json['actors']), len(json['concepts']), len(json['aa'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
