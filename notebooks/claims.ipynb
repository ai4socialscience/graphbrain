{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claims Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import gb.hypergraph.symbol as sym\n",
    "import gb.hypergraph.edge as ed\n",
    "import gb.nlp.parser as par\n",
    "import gb.tools.json as json_tools\n",
    "from gb.clusters.meronomy import Meronomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_PROB = -12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = par.Parser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data & build full edges list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edge_data = json_tools.read('../all.json')\n",
    "\n",
    "full_edges = []\n",
    "for it in edge_data:\n",
    "    full_edges.append(ed.without_namespaces(ed.str2edge(it['edge'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meronomy and Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build meronomy\n",
    "mer = Meronomy(parser, full_edges)\n",
    "mer.normalize_graph()\n",
    "\n",
    "# generate synonyms\n",
    "mer.generate_synonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter claim hyperedges\n",
    "(says x ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rel_contains(full_edge, term):\n",
    "    if sym.is_edge(full_edge) and len(full_edge) > 2 and sym.is_edge(full_edge[2]):\n",
    "        rel = full_edge[0]\n",
    "        if sym.is_edge(rel):\n",
    "            return term in rel\n",
    "        else:\n",
    "            return rel == term\n",
    "    return False\n",
    "\n",
    "\n",
    "say_edges = []\n",
    "for full_edge in full_edges:\n",
    "    if rel_contains(full_edge, 'says'):\n",
    "        say_edges.append(full_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def edge2str(edge):\n",
    "    s = ed.edge2str(edge, namespaces=False)\n",
    "    if sym.is_edge(edge):\n",
    "        return s\n",
    "\n",
    "    if s[0] == '+':\n",
    "        s = s[1:]\n",
    "\n",
    "    if len(s) == 0:\n",
    "        return None\n",
    "\n",
    "    if not s[0].isalnum():\n",
    "        return None\n",
    "\n",
    "    word = parser.make_word(s)\n",
    "    if word.prob < MAX_PROB:\n",
    "        return s\n",
    "\n",
    "    return None\n",
    "\n",
    "def edge2syn(edge):\n",
    "    atom = edge2str(edge)\n",
    "    if atom:\n",
    "        syn_id = mer.syn_id(atom)\n",
    "        if syn_id:\n",
    "            return syn_id\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sayers, sayers + claims & sorted sayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sayers = {}\n",
    "sayers_and_claims = {}\n",
    "for edge in say_edges:\n",
    "    sayer = edge2syn(edge[1])\n",
    "    if sayer not in sayers_and_claims:\n",
    "        sayers[sayer] = 0\n",
    "        sayers_and_claims[sayer] = []\n",
    "    sayers[sayer] += 1\n",
    "    sayers_and_claims[sayer].append(edge[2])\n",
    "        \n",
    "sorted_sayers = sorted(sayers.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepts by sayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concepts_by_sayer = {}\n",
    "\n",
    "\n",
    "def add_concepts(targ, src):\n",
    "    for key in src:\n",
    "        if key in targ:\n",
    "            targ[key] += src[key]\n",
    "        else:\n",
    "            targ[key] = src[key]\n",
    "\n",
    "\n",
    "def concepts_in_claim(claim, concept_map=None):\n",
    "    if not concept_map:\n",
    "        concept_map = {}\n",
    "    syn_id = edge2syn(claim)\n",
    "    if syn_id:\n",
    "        if syn_id not in concept_map:\n",
    "            concept_map[syn_id] = 0\n",
    "        concept_map[syn_id] += 1\n",
    "        \n",
    "        if sym.is_edge(claim):\n",
    "            for item in claim:\n",
    "                concepts_in_claim(item, concept_map)\n",
    "    return concept_map\n",
    "\n",
    "\n",
    "def get_concepts_by_sayer(sayer, that_include=None):\n",
    "    concept_map = {}\n",
    "    for claim in sayers_and_claims[sayer]:\n",
    "        claim_concepts = concepts_in_claim(claim)\n",
    "        if not that_include:\n",
    "            add_concepts(concept_map, claim_concepts)\n",
    "        elif that_include in claim_concepts.keys():\n",
    "            del claim_concepts[that_include]\n",
    "            add_concepts(concept_map, claim_concepts)\n",
    "    return concept_map\n",
    "\n",
    "\n",
    "for sayer in sayers_and_claims:\n",
    "    concepts_by_sayer[sayer] = get_concepts_by_sayer(sayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who talks about who graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "who_who = {}\n",
    "\n",
    "\n",
    "def add_edge(orig, targ):\n",
    "    if orig and targ:\n",
    "        if orig not in who_who:\n",
    "            who_who[orig] = {}\n",
    "        if targ not in who_who[orig]:\n",
    "            who_who[orig][targ] = 0\n",
    "        who_who[orig][targ] += 1\n",
    "\n",
    "for sayer in sayers:\n",
    "    for claim in sayers_and_claims[sayer]:\n",
    "        claim_concepts = concepts_in_claim(claim)\n",
    "        for concept in claim_concepts:\n",
    "            if concept in sayers:\n",
    "                add_edge(sayer, concept)\n",
    "            \n",
    "file = open('../who_who.csv', 'w')\n",
    "for orig in who_who:\n",
    "    for targ in who_who[orig]:\n",
    "        w = who_who[orig][targ]\n",
    "        file.write('%s,%s,%s\\n' % (mer.synonym_label(orig, short=True), mer.synonym_label(targ, short=True), w))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 {donald, trump, (+ donald trump)} 358 1479\n",
      "8 {(+ bernie sanders), sanders, bernie} 93 369\n",
      "20 {hillary, (+ hillary clinton), clinton} 88 338\n",
      "258749 {obama} 75 304\n",
      "764 {ryan, (+ paul ryan), paul} 21 95\n",
      "453 {mike, (+ mike pence), pence} 17 70\n",
      "51 {(+ john kasich), john, kasich} 15 62\n",
      "1 {(+ gary johnson), johnson, (+libertarian (+ gary johnson))} 14 44\n",
      "265004 {(+the latest)} 14 58\n",
      "259357 {(+ white house)} 13 55\n",
      "151 {(+the fbi), fbi} 11 39\n",
      "504 {marco, rubio, (+ marco rubio)} 11 39\n",
      "723 {chris, (+ chris christie), christie} 9 28\n",
      "265734 {(+ bill clinton)} 9 40\n",
      "260899 {(+ trump campaign)} 9 38\n",
      "842 {mitch, (+ mitch mcconnell), mcconnell} 9 39\n",
      "810 {(+ rudy giuliani), giuliani, rudy} 8 33\n",
      "234 {newt, gingrich, (+ newt gingrich)} 8 38\n",
      "208 {warren, elizabeth, (+ elizabeth warren)} 7 36\n"
     ]
    }
   ],
   "source": [
    "for t in sorted_sayers[:20]:\n",
    "    syn_id = t[0]\n",
    "    if syn_id:\n",
    "        print('%s %s %s %s' % (syn_id, mer.synonym_label(syn_id), t[1], len(concepts_by_sayer[syn_id])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{donald, trump, (+ donald trump)} 28\n",
      "{(+ bernie sanders), sanders, bernie} 25\n",
      "{(+of use (+a (+private (+ email server)))), (+a (+private (+ email server))), (+for investigation (+her (+of use (+a (+private (+ email server)))))), (+private (+ email server)), (+federal (+for investigation (+her (+of use (+a (+private (+ email server))))))), (+ email server), (+under (+federal (+for investigation (+her (+of use (+a (+private (+ email server)))))))), (+over (+her (+of use (+a (+private (+ email server)))))), (+her (+of use (+a (+private (+ email server)))))} 6\n",
      "{(+about (+hard (+' choices))), (+hard (+' choices)), (+' choices), (+but it (+about (+hard (+' choices))))} 4\n",
      "{(+ his homework’), homework’, (+ bank regulation), (+on (+ his homework’) (+ bank regulation))} 4\n",
      "{paranoia, (+and paranoia prejudice), prejudice, (+on (+and paranoia prejudice))} 4\n",
      "{(+$ 200,000), 200,000, (+in (+just (+more (+than (+$ 200,000)))) income), (+about (+$ 200,000)), (+than (+$ 200,000)), (+more (+than (+$ 200,000))), (+just (+more (+than (+$ 200,000)))), (+tickets (+$ 200,000))} 3\n",
      "{(+ water crisis), (+in (+the (+flint (+ water crisis)))), (+flint (+ water crisis)), (+the (+flint (+ water crisis))), flint} 3\n",
      "{(+was_careless it), was_careless, (+believes (+was_careless it))} 3\n",
      "{(+the fbi), fbi} 3\n",
      "{(+by (+national (+ news groups))), (+ news groups), (+national (+ news groups))} 3\n",
      "{(+a (+of world fantasy)), (+in (+a (+of world fantasy))), (+of world fantasy)} 3\n",
      "{(+ her vote), (+a (+ donor influence) (+ her vote)), (+ donor influence)} 3\n",
      "{(+ record 90%), (+a (+ record 90%) (+of americans)), (have (+a (+ record 90%) (+of americans)) (+ health insurance) today)} 3\n",
      "{rooted_for, (+the housing crisis), (+rooted_for (+the housing crisis))} 3\n",
      "{(+terrorist (+ propaganda videos)), (+in (+terrorist (+ propaganda videos))), (+ propaganda videos)} 3\n",
      "{(+ climate denial), (+his (+ climate denial)), (+citing (+his (+ climate denial)))} 3\n",
      "{(is she (+target (+of investigation))), (+of investigation), (+target (+of investigation))} 3\n",
      "{putin} 3\n",
      "{(+'s (+ sexual past) bill), (+ sexual past), (+after (+'s (+ sexual past) bill)), (+if go we (+after (+'s (+ sexual past) bill)))} 2\n"
     ]
    }
   ],
   "source": [
    "concepts = concepts_by_sayer[20]\n",
    "\n",
    "sorted_concepts = sorted(concepts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "for t in sorted_concepts[:20]:\n",
    "    syn_id = t[0]\n",
    "    if syn_id:\n",
    "        print('%s %s' % (mer.synonym_label(syn_id), t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(+ gary johnson), johnson, (+libertarian (+ gary johnson))}\n",
      "{donald, trump, (+ donald trump)}\n",
      "{putin}\n",
      "{gop}\n",
      "{(+ bernie sanders), sanders, bernie}\n",
      "{nominee}\n",
      "{vladimir, (+ vladimir putin)}\n",
      "{presidency, (+the presidency)}\n",
      "{(+in u.s)}\n",
      "{(+ climate change)}\n",
      "{hillary, (+ hillary clinton), clinton}\n",
      "{(+the fbi), fbi}\n",
      "{russia}\n",
      "{u.s, (+the u.s)}\n",
      "{shootings, (+ police shootings)}\n",
      "{presidential}\n",
      "{(+of (+daily (+ intelligence briefings))), (+ intelligence briefings), (+his (+daily (+ intelligence briefings))), briefings, (+daily (+ intelligence briefings)), (+to_skip (+daily (+ intelligence briefings)))}\n",
      "{meddling, (+for calls (+select (+on panel (+ russian meddling)))), (+ russian meddling), (+select (+on panel (+ russian meddling))), (+on panel (+ russian meddling))}\n",
      "{will_take}\n",
      "{african, (+ african americans)}\n",
      "{india}\n",
      "{democrats}\n",
      "{cnn}\n",
      "{(+of (+the (+ world leaders) (+who had))), (+the (+ world leaders) (+who had)), (+two (+of (+the (+ world leaders) (+who had)))), (+ world leaders), (+who had)}\n",
      "{(+ted cruz), ted}\n",
      "{will_be}\n",
      "{cruz}\n",
      "{americans}\n",
      "{probe}\n",
      "{russian}\n"
     ]
    }
   ],
   "source": [
    "# common concepts\n",
    "\n",
    "concepts1 = set(concepts_by_sayer[2].keys())\n",
    "concepts2 = set(concepts_by_sayer[258749].keys())\n",
    "\n",
    "common = concepts1.intersection(concepts2)\n",
    "for concept in common:\n",
    "    print(mer.synonym_label(concept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(+and (+ bernie sanders) (+a communist))} 1\n",
      "{(+a communist)} 1\n",
      "{(+' sanders)} 2\n",
      "{(will_debate he (+ bernie sanders) (+for (+$ (+10 million))))} 1\n",
      "{will_debate, (will_debate in)} 1\n",
      "{(+for (+$ (+10 million)))} 1\n",
      "{(+$ (+10 million)), (+than (+$ (+10 million))), (+10 million), (+more (+than (+$ (+10 million))))} 2\n",
      "{(would_kill (+ bernie sanders) golf (+with high taxes))} 1\n",
      "{(thought would_kill), ((thought would_kill) economists obamacare jobs), would_kill} 1\n",
      "{(+with high taxes)} 1\n",
      "{('ll_order he supporters (+to_disrupt (+rallies (+' sanders))))} 1\n",
      "{(+to_disrupt (+rallies (+' sanders)))} 1\n",
      "{to_disrupt, (+to_disrupt (+ pipeline drilling)), (+ pipeline drilling)} 1\n",
      "{(+rallies (+' sanders))} 1\n",
      "{rallies} 1\n",
      "{(+clinton says not_qualified’ because sanders so)} 1\n",
      "{hillary, (+ hillary clinton), clinton} 1\n",
      "{not_qualified’} 1\n",
      "{(says sanders (+no debate) (+ interested networks))} 1\n",
      "{(+no debate)} 1\n",
      "{(+ interested networks)} 1\n",
      "{(sold (+ bernie sanders) (+ his soul) (+to (+the devil’)))} 1\n",
      "{(+ his soul)} 1\n",
      "{devil’, (+to (+the devil’)), (+the devil’)} 3\n"
     ]
    }
   ],
   "source": [
    "concepts = get_concepts_by_sayer(2, that_include=8)\n",
    "for concept in concepts:\n",
    "    print('%s %s' % (mer.synonym_label(concept), concepts[concept]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
