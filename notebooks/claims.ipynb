{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Claims Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "import math\n",
    "from gb.hypergraph.hypergraph import HyperGraph\n",
    "import gb.hypergraph.symbol as sym\n",
    "import gb.hypergraph.edge as ed\n",
    "import gb.nlp.parser as par\n",
    "import gb.tools.json as json_tools\n",
    "from gb.synonyms.meronomy import Meronomy\n",
    "from gb.metrics.hyper_similarity import HyperSimilarity\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_PROB = -12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hg = HyperGraph({'backend': 'leveldb', 'hg': '../reddit-worldnews-01012013-01082017.hg'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = par.Parser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aux Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rel_contains(full_edge, term):\n",
    "    if sym.is_edge(full_edge) and len(full_edge) > 2 and sym.is_edge(full_edge[2]):\n",
    "        rel = full_edge[0]\n",
    "        if sym.is_edge(rel):\n",
    "            return term in rel\n",
    "        else:\n",
    "            return rel == term\n",
    "    return False\n",
    "\n",
    "\n",
    "def edge2str(edge):\n",
    "    s = ed.edge2str(edge, namespaces=False)\n",
    "    if sym.is_edge(edge):\n",
    "        return s\n",
    "\n",
    "    if s[0] == '+':\n",
    "        s = s[1:]\n",
    "\n",
    "    if len(s) == 0:\n",
    "        return None\n",
    "\n",
    "    if not s[0].isalnum():\n",
    "        return None\n",
    "\n",
    "    word = parser.make_word(s)\n",
    "    if word.prob < MAX_PROB:\n",
    "        return s\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def edge2syn(edge):\n",
    "    atom = edge2str(edge)\n",
    "    if atom:\n",
    "        syn_id = mer.syn_id(atom)\n",
    "        if syn_id:\n",
    "            return syn_id\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meronomy and Say Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mer = Meronomy(parser)\n",
    "\n",
    "edge_data = json_tools.read('../all-reddit-worldnews-01012013-01082017.json')\n",
    "\n",
    "say_edges = []\n",
    "for it in edge_data:\n",
    "    edge_ns = ed.str2edge(it['edge'])\n",
    "    mer.add_edge(edge_ns)\n",
    "    edge = ed.without_namespaces(edge_ns)\n",
    "    \n",
    "    # (says x ...)\n",
    "    if rel_contains(edge, 'says'):\n",
    "        say_edges.append(edge)\n",
    "        \n",
    "mer.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sayers, sayers + claims & sorted sayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sayers = {}\n",
    "sayers_and_claims = {}\n",
    "for edge in say_edges:\n",
    "    sayer = edge2syn(edge[1])\n",
    "    if sayer not in sayers_and_claims:\n",
    "        sayers[sayer] = 0\n",
    "        sayers_and_claims[sayer] = []\n",
    "    sayers[sayer] += 1\n",
    "    sayers_and_claims[sayer].append(edge[2])\n",
    "        \n",
    "sorted_sayers = sorted(sayers.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepts by sayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concepts_by_sayer = {}\n",
    "\n",
    "\n",
    "def add_concepts(targ, src):\n",
    "    for key in src:\n",
    "        if key in targ:\n",
    "            targ[key] += src[key]\n",
    "        else:\n",
    "            targ[key] = src[key]\n",
    "\n",
    "\n",
    "def concepts_in_claim(claim, concept_map=None, deep=True):\n",
    "    if not concept_map:\n",
    "        concept_map = {}\n",
    "    syn_id = edge2syn(claim)\n",
    "    if syn_id:\n",
    "        if syn_id not in concept_map:\n",
    "            concept_map[syn_id] = 0\n",
    "        concept_map[syn_id] += 1\n",
    "        \n",
    "        if deep:\n",
    "            if sym.is_edge(claim):\n",
    "                for item in claim:\n",
    "                    concepts_in_claim(item, concept_map)\n",
    "    return concept_map\n",
    "\n",
    "\n",
    "def get_concepts_by_sayer(sayer, that_include=None):\n",
    "    concept_map = {}\n",
    "    for claim in sayers_and_claims[sayer]:\n",
    "        claim_concepts = concepts_in_claim(claim)\n",
    "        if not that_include:\n",
    "            add_concepts(concept_map, claim_concepts)\n",
    "        elif that_include in claim_concepts.keys():\n",
    "            del claim_concepts[that_include]\n",
    "            add_concepts(concept_map, claim_concepts)\n",
    "    return concept_map\n",
    "\n",
    "\n",
    "for sayer in sayers_and_claims:\n",
    "    concepts_by_sayer[sayer] = get_concepts_by_sayer(sayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who talks about who graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "who_who = {}\n",
    "\n",
    "actors = [sayer for sayer in sayers if sayer and len(sayers_and_claims[sayer]) > 1]\n",
    "active_actors = set()\n",
    "\n",
    "def add_edge(orig, targ):\n",
    "    if orig and targ:\n",
    "        if orig not in who_who:\n",
    "            who_who[orig] = {}\n",
    "        if targ not in who_who[orig]:\n",
    "            who_who[orig][targ] = 0\n",
    "        who_who[orig][targ] += 1\n",
    "        if orig != targ:\n",
    "            active_actors.add(orig)\n",
    "            active_actors.add(targ)\n",
    "\n",
    "for sayer in actors:\n",
    "    for claim in sayers_and_claims[sayer]:\n",
    "        claim_concepts = concepts_in_claim(claim)\n",
    "        for concept in claim_concepts:\n",
    "            if concept in actors:\n",
    "                add_edge(sayer, concept)\n",
    "            \n",
    "file = open('../who_who.gml', 'w')\n",
    "file.write('graph\\n[\\n')\n",
    "for actor in active_actors:\n",
    "    # print(actor)\n",
    "    file.write('node\\n[\\nid %s\\nlabel \"%s\"\\n]\\n' % (str(actor), mer.synonym_label(actor, short=True)))\n",
    "for orig in who_who:\n",
    "    for targ in who_who[orig]:\n",
    "        w = who_who[orig][targ]\n",
    "        file.write('edge\\n[\\nsource %s\\ntarget %s\\nweight %s\\n]\\n' % (str(orig), str(targ), str(w)))\n",
    "file.write(']\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept-actor graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concept_actor = {}\n",
    "\n",
    "def add_edge(orig, targ):\n",
    "    if orig and targ:\n",
    "        if orig not in concept_actor:\n",
    "            concept_actor[orig] = {}\n",
    "        if targ not in concept_actor[orig]:\n",
    "            concept_actor[orig][targ] = 0\n",
    "        concept_actor[orig][targ] += 1\n",
    "\n",
    "def actors_for_concept(concept):\n",
    "    return concept_actor[concept].keys()\n",
    "        \n",
    "for sayer in actors:\n",
    "    for claim in sayers_and_claims[sayer]:\n",
    "        claim_concepts = concepts_in_claim(claim)\n",
    "        for concept in claim_concepts:\n",
    "            add_edge(concept, sayer)\n",
    "\n",
    "concept_metrics = {}\n",
    "            \n",
    "for concept in concept_actor:\n",
    "    weights = [concept_actor[concept][actor] for actor in concept_actor[concept]]\n",
    "    total = sum(weights)\n",
    "    h_weights = [float(i) / float(total) for i in weights]\n",
    "    h_weights = [i * i for i in h_weights]\n",
    "    h = 1. / sum(h_weights)\n",
    "    concept_metrics[concept] = {'total': total, 'h': h}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispersion of Concepts amongst Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [concept_metrics[concept]['h'] for concept in concept_metrics]\n",
    "x = [i for i in x if i < 25]\n",
    "\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(x, 50, log=True, facecolor='green', alpha=0.75)\n",
    "\n",
    "plt.xlabel('1 / herfindhal')\n",
    "plt.ylabel('Frequency (log)')\n",
    "plt.title('Dispersion of Concepts amongst Actors')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Top Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900878 {russia} 196 973\n",
      "900231 {iran} 110 457\n",
      "233 {(+ north korea), korea} 106 433\n",
      "900738 {putin} 92 446\n",
      "900119 {turkey} 70 355\n",
      "900854 {ukraine} 67 261\n",
      "845 {(+ pope francis), pope, francis} 59 301\n",
      "906466 {erdogan} 48 200\n",
      "1357 {(+ saudi arabia), saudi, arabia} 46 239\n",
      "900102 {syria} 44 175\n",
      "902006 {pakistan} 42 171\n",
      "900703 {france} 41 206\n",
      "914809 {(+ south korea)} 38 191\n",
      "801 {david, (+ david cameron), cameron} 34 156\n",
      "900223 {japan} 32 180\n",
      "455 {(+the kremlin), kremlin} 31 143\n",
      "39 {angela, merkel, (+ angela merkel)} 31 171\n",
      "900597 {trump} 30 126\n",
      "900095 {eu} 27 140\n"
     ]
    }
   ],
   "source": [
    "for t in sorted_sayers[:20]:\n",
    "    syn_id = t[0]\n",
    "    if syn_id:\n",
    "        print('%s %s %s %s' % (syn_id, mer.synonym_label(syn_id), t[1], len(concepts_by_sayer[syn_id])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepts by actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[473511] very grave sin 6 1.0\n",
      "[68156] of acts or terrorism fundamentalism 6 1.0\n",
      "[424530] 's amount world many conflicts 5 1.0\n",
      "[35745] is it wrong to_equate islam with violence 5 1.0\n",
      "[845] pope francis 4 2.9999999999999996\n",
      "[1925] catholic church 3 4.5\n",
      "[1897] the vatican 3 4.499999999999999\n",
      "[644031] to visit nation 3 1.0\n",
      "[174815] over rights their lands.pope 3 1.0\n",
      "[267135] as seeks trump to_advance its construction 3 1.0\n",
      "[260] on sunday 3 6.4\n",
      "[266480] as do they their mobile phones 3 1.0\n",
      "[121686] 17-month slide 3 1.0\n",
      "[657615] on war marriage 3 1.0\n",
      "[561989] by militants name unprecedented violence condemned islamist militants 3 1.0\n",
      "[18413] convicted pedophiles 2 1.0\n",
      "[1663] assisted suicide 2 1.0\n",
      "[905849] the church 2 1.0\n",
      "[384308] of state israel 2 1.0\n",
      "[425225] - migrant walls anti 2 1.0\n"
     ]
    }
   ],
   "source": [
    "concepts = concepts_by_sayer[845]\n",
    "\n",
    "sorted_concepts = sorted(concepts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "for t in sorted_concepts[:20]:\n",
    "    syn_id = t[0]\n",
    "    if syn_id:\n",
    "        print('[%s] %s %s %s' % (syn_id, mer.synonym_label(syn_id, short=True), t[1], concept_metrics[syn_id]['h']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepts by actor and $1/herfindhal$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1001589] global economy 2.0 {putin}\n",
      "[901435] the internet 2.0 {new un report}\n",
      "[915660] based_on 2.0 {unicef}\n",
      "[959419] contraception 2.0 {erdogan}\n",
      "[1781] hunger strike 2.0 {new un report}\n",
      "[268036] financial crisis 2.0 {putin}\n",
      "[10079] goodwill ambassador 2.0 {syrian opposition}\n",
      "[906866] dignity 1.7999999999999998 {north korea}\n",
      "[267378] guilty four men of_killing gypsy people including a child 2.0 {unicef}\n",
      "[902824] does_n't 2.0 {putin}\n",
      "[309] gas pipeline 2.0 {new poll}\n",
      "[904245] twitter 2.0 {'s president erdogan turkey}\n",
      "[1161796] will_try 2.0 {foreign secretary}\n",
      "[2401522] with donald trump 2.0 {japan}\n",
      "[16949] a martyr 2.0 {mahmoud abbas}\n",
      "[905749] priests 2.0 {the vatican}\n",
      "[922890] bishops 2.0 {syria}\n"
     ]
    }
   ],
   "source": [
    "ego = 845\n",
    "concepts = concepts_by_sayer[ego]\n",
    "\n",
    "concepts_by_h = {}\n",
    "for concept in concepts:\n",
    "    h = int(round(concept_metrics[concept]['h']))\n",
    "    if h not in concepts_by_h:\n",
    "        concepts_by_h[h] = []\n",
    "    concepts_by_h[h].append(concept)\n",
    "\n",
    "for syn_id in concepts_by_h[2][:20]:\n",
    "    actors = [mer.synonym_label(actor, short=True) for actor in actors_for_concept(syn_id) if actor != ego]\n",
    "    actor_str = ', '.join(actors)\n",
    "    print('[%s] %s %s {%s}' % (syn_id, mer.synonym_label(syn_id, short=True), concept_metrics[syn_id]['h'], actor_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment on concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego = 845         # the pope\n",
    "concept = 959230  # contraception\n",
    "\n",
    "def related_concepts(actor, concept):\n",
    "    result = set()\n",
    "    for claim in sayers_and_claims[actor]:\n",
    "        claim_concepts = concepts_in_claim(claim)\n",
    "        if concept in claim_concepts:\n",
    "            for c in claim_concepts:\n",
    "                if c != concept:\n",
    "                    result.add(c)\n",
    "    return result\n",
    "\n",
    "actors = actors_for_concept(concept)\n",
    "\n",
    "for actor in actors:\n",
    "    actor_name = mer.synonym_label(actor, short=True)\n",
    "    print('ACTOR: %s' % actor_name)\n",
    "    related = related_concepts(actor, concept)\n",
    "    for rel in related:\n",
    "        print(mer.synonym_label(rel, short=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who talks about what - Ego Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ego = 845\n",
    "who_what = {}\n",
    "\n",
    "concepts = concepts_by_sayer[ego]\n",
    "\n",
    "nodes = set()\n",
    "\n",
    "def add_edge(orig, targ, weight):\n",
    "    if orig and targ:\n",
    "        if orig not in who_what:\n",
    "            who_what[orig] = {}\n",
    "        who_what[orig][targ] = weight\n",
    "        nodes.add(orig)\n",
    "        nodes.add(targ)\n",
    "\n",
    "for concept in concepts:\n",
    "    if len(concept_actor[concept]) > 1:\n",
    "        for actor in concept_actor[concept]:\n",
    "            add_edge(actor, concept, concept_actor[concept][actor])\n",
    "        \n",
    "file = open('../who_what.gml', 'w')\n",
    "file.write('graph\\n[\\n')\n",
    "for node in nodes:\n",
    "    file.write('node\\n[\\nid %s\\nlabel \"%s\"\\n]\\n' % (str(node), mer.synonym_label(node, short=True)))\n",
    "for orig in who_what:\n",
    "    for targ in who_what[orig]:\n",
    "        w = who_what[orig][targ]\n",
    "        file.write('edge\\n[\\nsource %s\\ntarget %s\\nweight %s\\n]\\n' % (str(orig), str(targ), str(w)))\n",
    "file.write(']\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego = 900574\n",
    "\n",
    "concepts = concepts_by_sayer[ego]\n",
    "\n",
    "file = open('../france.csv', 'w')\n",
    "file.write('concept,herfindahl,total,ego_total,actors\\n')\n",
    "for concept in concepts:\n",
    "    file.write('%s,%s,%s,%s,' % (mer.synonym_label(concept, short=True),\n",
    "                                 concept_metrics[concept]['h'],\n",
    "                                 concept_metrics[concept]['total'],\n",
    "                                 concept_actor[concept][ego]))\n",
    "    actor_str = '|'.join([mer.synonym_label(actor, short=True) for actor in concept_actor[concept] if actor != ego])\n",
    "    file.write('%s\\n' % actor_str)\n",
    "file.write('\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common concepts\n",
    "\n",
    "actor1 = 845\n",
    "actor2 = 455\n",
    "\n",
    "concepts1 = set(concepts_by_sayer[actor1].keys())\n",
    "concepts2 = set(concepts_by_sayer[actor2].keys())\n",
    "\n",
    "common = concepts1.intersection(concepts2)\n",
    "for concept in common:\n",
    "    print('*** %s %s %s' % (mer.synonym_label(concept, short=True),\n",
    "                            concept_actor[concept][actor1],\n",
    "                            concept_actor[concept][actor2]))\n",
    "    for claim in sayers_and_claims[actor1]:\n",
    "        claim_concepts = concepts_in_claim(claim)\n",
    "        if concept in claim_concepts:\n",
    "            print('###')\n",
    "            for concept_ in claim_concepts:\n",
    "                if concept != concept_:\n",
    "                    # pass\n",
    "                    print('> %s %s' % (actor1, mer.synonym_label(concept_, short=True)))\n",
    "    for claim in sayers_and_claims[actor2]:\n",
    "        claim_concepts = concepts_in_claim(claim)\n",
    "        if concept in claim_concepts:\n",
    "            print('###')\n",
    "            for concept_ in claim_concepts:\n",
    "                if concept != concept_:\n",
    "                    # pass\n",
    "                    print('> %s %s' % (actor2, mer.synonym_label(concept_, short=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = get_concepts_by_sayer(2, that_include=19)\n",
    "for concept in concepts:\n",
    "    print('%s %s' % (mer.synonym_label(concept), concepts[concept]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALUE ERROR! invalid literal for int() with base 10: 'pope/nlp.pope.noun' perm2edge p(calls_for/nlp.call.verb+nlp.for.adp pope/nlp.pope.noun (+bold/nlp.bold.adj (+/gb cultural/nlp.cultural.adj revolution/nlp.revolution.noun) (+to_combat/nlp.to.part+nlp.combat.verb (+in/nlp.in.adp (+/gb climate/nlp.climate.noun change/nlp.change.noun) (+new/nlp.new.adj (+/gb teaching/nlp.teaching.noun document/nlp.document.noun) (+_/nlp.(.punct encyclical/nlp.encyclical.noun)))))) src/gb warp_waffle/reddit_user 2\n",
      "VALUE ERROR! invalid literal for int() with base 10: 'pope/nlp.pope.noun' perm2edge p(calls_for/nlp.call.verb+nlp.for.adp pope/nlp.pope.noun (+bold/nlp.bold.adj (+/gb cultural/nlp.cultural.adj revolution/nlp.revolution.noun) (+to_combat/nlp.to.part+nlp.combat.verb (+in/nlp.in.adp (+/gb climate/nlp.climate.noun change/nlp.change.noun) (+new/nlp.new.adj (+/gb teaching/nlp.teaching.noun document/nlp.document.noun) (+_/nlp.(.punct encyclical/nlp.encyclical.noun)))))) warp_waffle/reddit_user src/gb 3\n",
      "VALUE ERROR! invalid literal for int() with base 10: '(+middle/nlp.middle.noun east/nlp.east.adj peace/nlp.peace.noun)' perm2edge p((prays/nlp.pray.verb calls_for/nlp.call.verb+nlp.for.adp) (+/gb pope/nlp.pope.noun francis/nlp.francis.propn) (+at/nlp.at.adp bethlehem/nlp.bethlehem.noun (+between/nlp.between.adp wall/nlp.wall.noun (+and/nlp.and.conj (+state/nlp.state.noun israel/nlp.israel.propn (+of/nlp.of.adp palestine/nlp.palestine.noun))))) (+middle/nlp.middle.noun east/nlp.east.adj peace/nlp.peace.noun) (+_/nlp.(.punct photo/nlp.photo.noun)) src/gb thetripleb/reddit_user 2\n",
      "VALUE ERROR! invalid literal for int() with base 10: '(+middle/nlp.middle.noun east/nlp.east.adj peace/nlp.peace.noun)' perm2edge p((prays/nlp.pray.verb calls_for/nlp.call.verb+nlp.for.adp) (+/gb pope/nlp.pope.noun francis/nlp.francis.propn) (+at/nlp.at.adp bethlehem/nlp.bethlehem.noun (+between/nlp.between.adp wall/nlp.wall.noun (+and/nlp.and.conj (+state/nlp.state.noun israel/nlp.israel.propn (+of/nlp.of.adp palestine/nlp.palestine.noun))))) (+middle/nlp.middle.noun east/nlp.east.adj peace/nlp.peace.noun) (+_/nlp.(.punct photo/nlp.photo.noun)) thetripleb/reddit_user src/gb 3\n",
      "VALUE ERROR! invalid literal for int() with base 10: 'francis/nlp.francis.noun' perm2edge p(+pope/nlp.pope.noun (must_change_said/nlp.say.verb+nlp.must.verb+nlp.change.verb people/nlp.people.noun (+their/nlp.their.adj (+and/nlp.and.conj attitudes/nlp.attitude.noun lifestyles/nlp.lifestyle.noun)) (+to_help/nlp.to.part+nlp.help.verb (+defeat/nlp.defeat.verb hunger/nlp.hunger.noun))) francis/nlp.francis.noun (+thursday/nlp.thursday.noun (+_/nlp.(.punct (+11/nlp.11.num june/nlp.june.noun)) (+a/nlp.a.det (+of/nlp.of.adp hint/nlp.hint.noun (may_be_coming/nlp.may.verb+nlp.come.verb+nlp.be.verb what/nlp.what.noun (+in/nlp.in.adp (+his/nlp.his.adj (+/gb environmental/nlp.environmental.adj encyclical/nlp.encyclical.noun) (+much/nlp.much.adv anticipated/nlp.anticipate.verb))) (+/gb next/nlp.next.adj week/nlp.week.noun)))))) are_synonyms/gb people_must_change_their_lifestyles_and_attitudes_to_help_defeat_hunger_pope_francis_said_thursday___june_11_a_hint_of_what_may_be_coming_in_his_much_anticipated_environmental_encyclical_next_week/gb6f91499cb7ccc25d 2\n",
      "VALUE ERROR! invalid literal for int() with base 10: 'francis/nlp.francis.noun' perm2edge p(+pope/nlp.pope.noun (must_change_said/nlp.say.verb+nlp.must.verb+nlp.change.verb people/nlp.people.noun (+their/nlp.their.adj (+and/nlp.and.conj attitudes/nlp.attitude.noun lifestyles/nlp.lifestyle.noun)) (+to_help/nlp.to.part+nlp.help.verb (+defeat/nlp.defeat.verb hunger/nlp.hunger.noun))) francis/nlp.francis.noun (+thursday/nlp.thursday.noun (+_/nlp.(.punct (+11/nlp.11.num june/nlp.june.noun)) (+a/nlp.a.det (+of/nlp.of.adp hint/nlp.hint.noun (may_be_coming/nlp.may.verb+nlp.come.verb+nlp.be.verb what/nlp.what.noun (+in/nlp.in.adp (+his/nlp.his.adj (+/gb environmental/nlp.environmental.adj encyclical/nlp.encyclical.noun) (+much/nlp.much.adv anticipated/nlp.anticipate.verb))) (+/gb next/nlp.next.adj week/nlp.week.noun)))))) people_must_change_their_lifestyles_and_attitudes_to_help_defeat_hunger_pope_francis_said_thursday___june_11_a_hint_of_what_may_be_coming_in_his_much_anticipated_environmental_encyclical_next_week/gb6f91499cb7ccc25d are_synonyms/gb 3\n",
      "VALUE ERROR! invalid literal for int() with base 10: 'francis/nlp.francis.noun' perm2edge p(+pope/nlp.pope.noun (must_change_said/nlp.say.verb+nlp.must.verb+nlp.change.verb people/nlp.people.noun (+their/nlp.their.adj (+and/nlp.and.conj attitudes/nlp.attitude.noun lifestyles/nlp.lifestyle.noun)) (+to_help/nlp.to.part+nlp.help.verb (+defeat/nlp.defeat.verb hunger/nlp.hunger.noun))) francis/nlp.francis.noun (+thursday/nlp.thursday.noun (+_/nlp.(.punct (+11/nlp.11.num june/nlp.june.noun)) (+a/nlp.a.det (+of/nlp.of.adp hint/nlp.hint.noun (may_be_coming/nlp.may.verb+nlp.come.verb+nlp.be.verb what/nlp.what.noun (+in/nlp.in.adp (+his/nlp.his.adj (+/gb environmental/nlp.environmental.adj encyclical/nlp.encyclical.noun) (+much/nlp.much.adv anticipated/nlp.anticipate.verb))) (+/gb next/nlp.next.adj week/nlp.week.noun)))))) pnewell/reddit_user src/gb 3\n",
      "VALUE ERROR! invalid literal for int() with base 10: 'francis/nlp.francis.noun' perm2edge p(+pope/nlp.pope.noun (must_change_said/nlp.say.verb+nlp.must.verb+nlp.change.verb people/nlp.people.noun (+their/nlp.their.adj (+and/nlp.and.conj attitudes/nlp.attitude.noun lifestyles/nlp.lifestyle.noun)) (+to_help/nlp.to.part+nlp.help.verb (+defeat/nlp.defeat.verb hunger/nlp.hunger.noun))) francis/nlp.francis.noun (+thursday/nlp.thursday.noun (+_/nlp.(.punct (+11/nlp.11.num june/nlp.june.noun)) (+a/nlp.a.det (+of/nlp.of.adp hint/nlp.hint.noun (may_be_coming/nlp.may.verb+nlp.come.verb+nlp.be.verb what/nlp.what.noun (+in/nlp.in.adp (+his/nlp.his.adj (+/gb environmental/nlp.environmental.adj encyclical/nlp.encyclical.noun) (+much/nlp.much.adv anticipated/nlp.anticipate.verb))) (+/gb next/nlp.next.adj week/nlp.week.noun)))))) src/gb pnewell/reddit_user 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25.664712151938385"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs = HyperSimilarity(hg)\n",
    "\n",
    "c1 = 1001589   # global economy\n",
    "c2 = 901435    # the internet\n",
    "c3 = 268036    # financial crisis\n",
    "c4 = 905749    # priests\n",
    "c5 = 1925      # catholic church\n",
    "c6 = 845       # pope francis\n",
    "\n",
    "# mer.synonym_full_edges(c4)\n",
    "\n",
    "hs.synonym_similarity(mer, c5, c6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
